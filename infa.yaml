{
 "Description": "Creates the lab environment for the workshop",
 "Mappings": {
  "WorkshopAssetsBucketAssetsStaticBucketMapping": {
   "eu-north-1": {
    "bucketName": "ws-assets-prod-iad-r-arn-580aeca3990cef5a"
   },
   "ap-south-1": {
    "bucketName": "ws-assets-prod-iad-r-bom-431207042d319a2d"
   },
   "eu-west-3": {
    "bucketName": "ws-assets-prod-iad-r-cdg-9e76383c31ad6229"
   },
   "us-east-2": {
    "bucketName": "ws-assets-prod-iad-r-cmh-8d6e9c21a4dec77d"
   },
   "eu-west-1": {
    "bucketName": "ws-assets-prod-iad-r-dub-85e3be25bd827406"
   },
   "eu-central-1": {
    "bucketName": "ws-assets-prod-iad-r-fra-b129423e91500967"
   },
   "sa-east-1": {
    "bucketName": "ws-assets-prod-iad-r-gru-527b8c19222c1182"
   },
   "us-east-1": {
    "bucketName": "ws-assets-prod-iad-r-iad-ed304a55c2ca1aee"
   },
   "ap-northeast-2": {
    "bucketName": "ws-assets-prod-iad-r-icn-ced060f0d38bc0b0"
   },
   "ap-northeast-3": {
    "bucketName": "ws-assets-prod-iad-r-kix-c2a28ad4e55ea53a"
   },
   "eu-west-2": {
    "bucketName": "ws-assets-prod-iad-r-lhr-cc4472a651221311"
   },
   "ap-northeast-1": {
    "bucketName": "ws-assets-prod-iad-r-nrt-2cb4b4649d0e0f94"
   },
   "us-west-2": {
    "bucketName": "ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0"
   },
   "us-west-1": {
    "bucketName": "ws-assets-prod-iad-r-sfo-f61fc67057535f1b"
   },
   "ap-southeast-1": {
    "bucketName": "ws-assets-prod-iad-r-sin-694a125e41645312"
   },
   "ap-southeast-2": {
    "bucketName": "ws-assets-prod-iad-r-syd-b04c62a5f16f7b2e"
   },
   "ca-central-1": {
    "bucketName": "ws-assets-prod-iad-r-yul-5c2977cd61bca1f3"
   }
  }
 },
 "Resources": {
  "TFStateBackendBucketF0FC9A9D": {
   "Type": "AWS::S3::Bucket",
   "Properties": {
    "VersioningConfiguration": {
     "Status": "Enabled"
    }
   },
   "UpdateReplacePolicy": "Retain",
   "DeletionPolicy": "Retain"
  },
  "SharedRoleD1D02F7E": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": [
         "codebuild.amazonaws.com",
         "ec2.amazonaws.com",
         "glue.amazonaws.com"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/AdministratorAccess"
       ]
      ]
     },
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/AmazonSSMManagedInstanceCore"
       ]
      ]
     }
    ]
   }
  },
  "SharedRoleDefaultPolicyA7803C87": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "logs:CreateLogStream",
        "logs:PutLogEvents"
       ],
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "IDEFleetIdeLogGroup63888DED",
         "Arn"
        ]
       }
      },
      {
       "Action": [
        "secretsmanager:DescribeSecret",
        "secretsmanager:GetSecretValue"
       ],
       "Effect": "Allow",
       "Resource": {
        "Ref": "IDEFleetIdePasswordSecret4C89AB01"
       }
      },
      {
       "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSGITIAMStackDeployProjectE1461187"
           },
           ":*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSGITIAMStackDeployProjectE1461187"
           }
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSHubStackDeployProject89DD576F"
           },
           ":*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSHubStackDeployProject89DD576F"
           }
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSSpokeprodStackDeployProject510818EF"
           },
           ":*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSSpokeprodStackDeployProject510818EF"
           }
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSSpokestagingStackDeployProjectE3D9EDE7"
           },
           ":*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":logs:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":log-group:/aws/codebuild/",
           {
            "Ref": "EKSSpokestagingStackDeployProjectE3D9EDE7"
           }
          ]
         ]
        }
       ]
      },
      {
       "Action": [
        "codebuild:BatchPutCodeCoverages",
        "codebuild:BatchPutTestCases",
        "codebuild:CreateReport",
        "codebuild:CreateReportGroup",
        "codebuild:UpdateReport"
       ],
       "Effect": "Allow",
       "Resource": [
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":codebuild:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":report-group/",
           {
            "Ref": "EKSGITIAMStackDeployProjectE1461187"
           },
           "-*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":codebuild:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":report-group/",
           {
            "Ref": "EKSHubStackDeployProject89DD576F"
           },
           "-*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":codebuild:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":report-group/",
           {
            "Ref": "EKSSpokeprodStackDeployProject510818EF"
           },
           "-*"
          ]
         ]
        },
        {
         "Fn::Join": [
          "",
          [
           "arn:",
           {
            "Ref": "AWS::Partition"
           },
           ":codebuild:",
           {
            "Ref": "AWS::Region"
           },
           ":",
           {
            "Ref": "AWS::AccountId"
           },
           ":report-group/",
           {
            "Ref": "EKSSpokestagingStackDeployProjectE3D9EDE7"
           },
           "-*"
          ]
         ]
        }
       ]
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "SharedRoleDefaultPolicyA7803C87",
    "Roles": [
     {
      "Ref": "SharedRoleD1D02F7E"
     }
    ]
   }
  },
  "IDEFleetIdeVPCA4D0815F": {
   "Type": "AWS::EC2::VPC",
   "Properties": {
    "CidrBlock": "192.168.0.0/16",
    "EnableDnsHostnames": true,
    "EnableDnsSupport": true,
    "InstanceTenancy": "default",
    "Tags": [
     {
      "Key": "Name",
      "Value": "fleet-workshop/IDE-Fleet/IdeVPC"
     }
    ]
   }
  },
  "IDEFleetIdeVPCPublicSubnet1Subnet9C1901A4": {
   "Type": "AWS::EC2::Subnet",
   "Properties": {
    "AvailabilityZone": {
     "Fn::Select": [
      0,
      {
       "Fn::GetAZs": ""
      }
     ]
    },
    "CidrBlock": "192.168.0.0/24",
    "MapPublicIpOnLaunch": true,
    "Tags": [
     {
      "Key": "aws-cdk:subnet-name",
      "Value": "Public"
     },
     {
      "Key": "aws-cdk:subnet-type",
      "Value": "Public"
     },
     {
      "Key": "Name",
      "Value": "fleet-workshop/IDE-Fleet/IdeVPC/PublicSubnet1"
     }
    ],
    "VpcId": {
     "Ref": "IDEFleetIdeVPCA4D0815F"
    }
   }
  },
  "IDEFleetIdeVPCPublicSubnet1RouteTableDE9DC9F3": {
   "Type": "AWS::EC2::RouteTable",
   "Properties": {
    "Tags": [
     {
      "Key": "Name",
      "Value": "fleet-workshop/IDE-Fleet/IdeVPC/PublicSubnet1"
     }
    ],
    "VpcId": {
     "Ref": "IDEFleetIdeVPCA4D0815F"
    }
   }
  },
  "IDEFleetIdeVPCPublicSubnet1RouteTableAssociation949950FD": {
   "Type": "AWS::EC2::SubnetRouteTableAssociation",
   "Properties": {
    "RouteTableId": {
     "Ref": "IDEFleetIdeVPCPublicSubnet1RouteTableDE9DC9F3"
    },
    "SubnetId": {
     "Ref": "IDEFleetIdeVPCPublicSubnet1Subnet9C1901A4"
    }
   }
  },
  "IDEFleetIdeVPCPublicSubnet1DefaultRoute2D8CADD9": {
   "Type": "AWS::EC2::Route",
   "Properties": {
    "DestinationCidrBlock": "0.0.0.0/0",
    "GatewayId": {
     "Ref": "IDEFleetIdeVPCIGWDA3B7804"
    },
    "RouteTableId": {
     "Ref": "IDEFleetIdeVPCPublicSubnet1RouteTableDE9DC9F3"
    }
   },
   "DependsOn": [
    "IDEFleetIdeVPCVPCGW0D377588"
   ]
  },
  "IDEFleetIdeVPCIGWDA3B7804": {
   "Type": "AWS::EC2::InternetGateway",
   "Properties": {
    "Tags": [
     {
      "Key": "Name",
      "Value": "fleet-workshop/IDE-Fleet/IdeVPC"
     }
    ]
   }
  },
  "IDEFleetIdeVPCVPCGW0D377588": {
   "Type": "AWS::EC2::VPCGatewayAttachment",
   "Properties": {
    "InternetGatewayId": {
     "Ref": "IDEFleetIdeVPCIGWDA3B7804"
    },
    "VpcId": {
     "Ref": "IDEFleetIdeVPCA4D0815F"
    }
   }
  },
  "IDEFleetIdePrefixListFunctionServiceRoleDC99231D": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "IDEFleetIdePrefixListFunctionServiceRoleDefaultPolicyCCB0FA99": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "ec2:DescribeManagedPrefixLists",
       "Effect": "Allow",
       "Resource": "*"
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "IDEFleetIdePrefixListFunctionServiceRoleDefaultPolicyCCB0FA99",
    "Roles": [
     {
      "Ref": "IDEFleetIdePrefixListFunctionServiceRoleDC99231D"
     }
    ]
   }
  },
  "IDEFleetIdePrefixListFunction66D13E34": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "from __future__ import print_function\nimport boto3\nimport traceback\nimport cfnresponse\n\ndef lambda_handler(event, context):\n    print('Event: {}'.format(event))\n    print('context: {}'.format(context))\n    responseData = {}\n\n    status = cfnresponse.SUCCESS\n\n    if event['RequestType'] == 'Delete':\n        responseData = {'Success': 'Custom Resource removed'}\n        cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')\n    else:\n        try:\n            # Open AWS clients\n            ec2 = boto3.client('ec2')\n\n            res = ec2.describe_managed_prefix_lists(\n               Filters=[{\n                  'Name': 'prefix-list-name',\n                  'Values': ['com.amazonaws.global.cloudfront.origin-facing']\n               }]\n            )\n\n            responseData = {'PrefixListId': str(res['PrefixLists'][0]['PrefixListId'])}\n        except Exception as e:\n            status = cfnresponse.FAILED\n            tb_err = traceback.format_exc()\n            print(tb_err)\n            responseData = {'Error': tb_err}\n        finally:\n            cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')"
    },
    "Handler": "index.lambda_handler",
    "Role": {
     "Fn::GetAtt": [
      "IDEFleetIdePrefixListFunctionServiceRoleDC99231D",
      "Arn"
     ]
    },
    "Runtime": "python3.12",
    "Timeout": 180
   },
   "DependsOn": [
    "IDEFleetIdePrefixListFunctionServiceRoleDefaultPolicyCCB0FA99",
    "IDEFleetIdePrefixListFunctionServiceRoleDC99231D"
   ]
  },
  "IDEFleetIdePrefixListResourceF67673AB": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "IDEFleetIdePrefixListFunction66D13E34",
      "Arn"
     ]
    }
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "IDEFleetIdeSecurityGroup7FF3E772": {
   "Type": "AWS::EC2::SecurityGroup",
   "Properties": {
    "GroupDescription": "IDE security group",
    "SecurityGroupEgress": [
     {
      "CidrIp": "0.0.0.0/0",
      "Description": "Allow all outbound traffic by default",
      "IpProtocol": "-1"
     }
    ],
    "SecurityGroupIngress": [
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "IDEFleetIdeVPCA4D0815F",
        "CidrBlock"
       ]
      },
      "Description": "Gitea API from VPC",
      "FromPort": 9999,
      "IpProtocol": "tcp",
      "ToPort": 9999
     },
     {
      "CidrIp": {
       "Fn::GetAtt": [
        "IDEFleetIdeVPCA4D0815F",
        "CidrBlock"
       ]
      },
      "Description": "Gitea SSH from VPC",
      "FromPort": 2222,
      "IpProtocol": "tcp",
      "ToPort": 2222
     }
    ],
    "VpcId": {
     "Ref": "IDEFleetIdeVPCA4D0815F"
    }
   }
  },
  "IDEFleetIdeSecurityGroupfromIndirectPeer80DF0487D7": {
   "Type": "AWS::EC2::SecurityGroupIngress",
   "Properties": {
    "Description": "HTTP from CloudFront only",
    "FromPort": 80,
    "GroupId": {
     "Fn::GetAtt": [
      "IDEFleetIdeSecurityGroup7FF3E772",
      "GroupId"
     ]
    },
    "IpProtocol": "tcp",
    "SourcePrefixListId": {
     "Fn::GetAtt": [
      "IDEFleetIdePrefixListResourceF67673AB",
      "PrefixListId"
     ]
    },
    "ToPort": 80
   }
  },
  "IDEFleetInstanceProfile1BF87146": {
   "Type": "AWS::IAM::InstanceProfile",
   "Properties": {
    "Roles": [
     {
      "Ref": "SharedRoleD1D02F7E"
     }
    ]
   },
   "DependsOn": [
    "IDEFleetIdeVPCIGWDA3B7804",
    "IDEFleetIdeVPCPublicSubnet1DefaultRoute2D8CADD9",
    "IDEFleetIdeVPCPublicSubnet1RouteTableDE9DC9F3",
    "IDEFleetIdeVPCPublicSubnet1RouteTableAssociation949950FD",
    "IDEFleetIdeVPCPublicSubnet1Subnet9C1901A4",
    "IDEFleetIdeVPCA4D0815F",
    "IDEFleetIdeVPCVPCGW0D377588"
   ]
  },
  "IDEFleet0EC23AB4641a0cee2d30d2eb": {
   "Type": "AWS::EC2::Instance",
   "Properties": {
    "AvailabilityZone": {
     "Fn::Select": [
      0,
      {
       "Fn::GetAZs": ""
      }
     ]
    },
    "BlockDeviceMappings": [
     {
      "DeviceName": "/dev/xvda",
      "Ebs": {
       "DeleteOnTermination": true,
       "Encrypted": true,
       "VolumeSize": 30,
       "VolumeType": "gp3"
      }
     }
    ],
    "IamInstanceProfile": {
     "Ref": "IDEFleetInstanceProfile1BF87146"
    },
    "ImageId": {
     "Ref": "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter"
    },
    "InstanceType": "t3.medium",
    "NetworkInterfaces": [
     {
      "AssociatePublicIpAddress": true,
      "DeviceIndex": "0",
      "GroupSet": [
       {
        "Fn::GetAtt": [
         "IDEFleetIdeSecurityGroup7FF3E772",
         "GroupId"
        ]
       }
      ],
      "SubnetId": {
       "Ref": "IDEFleetIdeVPCPublicSubnet1Subnet9C1901A4"
      }
     }
    ],
    "Tags": [
     {
      "Key": "Name",
      "Value": "fleet-workshop/IDE-Fleet/IDE-Fleet"
     }
    ],
    "UserData": {
     "Fn::Base64": "#!/bin/bash"
    }
   },
   "DependsOn": [
    "IDEFleetIdeVPCIGWDA3B7804",
    "IDEFleetIdeVPCPublicSubnet1DefaultRoute2D8CADD9",
    "IDEFleetIdeVPCPublicSubnet1RouteTableDE9DC9F3",
    "IDEFleetIdeVPCPublicSubnet1RouteTableAssociation949950FD",
    "IDEFleetIdeVPCPublicSubnet1Subnet9C1901A4",
    "IDEFleetIdeVPCA4D0815F",
    "IDEFleetIdeVPCVPCGW0D377588",
    "SharedRoleDefaultPolicyA7803C87",
    "SharedRoleD1D02F7E"
   ]
  },
  "IDEFleetIdeBootstrapFunctionServiceRoleF314F5FF": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "IDEFleetIdeBootstrapFunctionServiceRoleDefaultPolicy297016DA": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "iam:PassRole",
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "SharedRoleD1D02F7E",
         "Arn"
        ]
       }
      },
      {
       "Action": [
        "ec2:DescribeInstances",
        "iam:ListInstanceProfiles",
        "ssm:DescribeInstanceInformation",
        "ssm:GetCommandInvocation",
        "ssm:SendCommand"
       ],
       "Effect": "Allow",
       "Resource": "*"
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "IDEFleetIdeBootstrapFunctionServiceRoleDefaultPolicy297016DA",
    "Roles": [
     {
      "Ref": "IDEFleetIdeBootstrapFunctionServiceRoleF314F5FF"
     }
    ]
   }
  },
  "IDEFleetIdeBootstrapFunctionC79D571D": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "from __future__ import print_function\nimport boto3\nimport json\nimport os\nimport time\nimport traceback\nimport cfnresponse\nfrom botocore.exceptions import WaiterError\n\ndef lambda_handler(event, context):\n    print('Event: {}'.format(event))\n    print('context: {}'.format(context))\n    responseData = {}\n\n    status = cfnresponse.SUCCESS\n\n    if event['RequestType'] == 'Delete':\n        responseData = {'Success': 'Custom Resource removed'}\n        cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')\n    else:\n        try:\n            # Open AWS clients\n            ec2 = boto3.client('ec2')\n            ssm = boto3.client('ssm')\n\n            instance_id = event['ResourceProperties']['InstanceId']\n\n            print('Waiting for the instance to be ready...')\n            # Wait for Instance to become ready\n            instance_state = 'unknown'\n            print('Instance is currently in state'.format(instance_state))\n            while instance_state != 'running':\n                time.sleep(5)\n                di = ec2.describe_instances(InstanceIds=[instance_id])\n                instance_state = di['Reservations'][0]['Instances'][0]['State']['Name']\n                print('Waiting for instance in state: {}'.format(instance_state))\n\n            print('Instance is ready')\n\n            print('Waiting for instance to come online in SSM...')\n            for i in range(1, 60):\n              response = ssm.describe_instance_information(Filters=[{'Key': 'InstanceIds', 'Values': [instance_id]}])\n              if len(response[\"InstanceInformationList\"]) == 0:\n                print('No instances in SSM')\n              elif len(response[\"InstanceInformationList\"]) > 0 and \\\n                    response[\"InstanceInformationList\"][0][\"PingStatus\"] == \"Online\" and \\\n                    response[\"InstanceInformationList\"][0][\"InstanceId\"] == instance_id:\n                print('Instance is online in SSM')\n                break\n              time.sleep(10)\n\n            ssm_document = event['ResourceProperties']['SsmDocument']\n\n            ssm.send_command(\n                InstanceIds=[instance_id],\n                DocumentName=ssm_document,\n                CloudWatchOutputConfig={\n                    'CloudWatchLogGroupName': event['ResourceProperties']['LogGroupName'],\n                    'CloudWatchOutputEnabled': True\n                })\n\n            responseData = {'Success': 'Started bootstrapping for instance: '+instance_id}\n        except Exception as e:\n            status = cfnresponse.FAILED\n            tb_err = traceback.format_exc()\n            print(tb_err)\n            responseData = {'Error': tb_err}\n        finally:\n            cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')"
    },
    "Handler": "index.lambda_handler",
    "Role": {
     "Fn::GetAtt": [
      "IDEFleetIdeBootstrapFunctionServiceRoleF314F5FF",
      "Arn"
     ]
    },
    "Runtime": "python3.12",
    "Timeout": 900
   },
   "DependsOn": [
    "IDEFleetIdeBootstrapFunctionServiceRoleDefaultPolicy297016DA",
    "IDEFleetIdeBootstrapFunctionServiceRoleF314F5FF"
   ]
  },
  "IDEFleetIdeLogGroup63888DED": {
   "Type": "AWS::Logs::LogGroup",
   "Properties": {
    "RetentionInDays": 7
   },
   "UpdateReplacePolicy": "Retain",
   "DeletionPolicy": "Retain"
  },
  "IDEFleetIdePasswordSecret4C89AB01": {
   "Type": "AWS::SecretsManager::Secret",
   "Properties": {
    "GenerateSecretString": {
     "ExcludeCharacters": "\"@/\\",
     "ExcludePunctuation": true,
     "GenerateStringKey": "password",
     "IncludeSpace": false,
     "PasswordLength": 32,
     "SecretStringTemplate": "{\"password\":\"\"}"
    }
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "IDEFleetIdeDistribution631906CF": {
   "Type": "AWS::CloudFront::Distribution",
   "Properties": {
    "DistributionConfig": {
     "DefaultCacheBehavior": {
      "AllowedMethods": [
       "GET",
       "HEAD",
       "OPTIONS",
       "PUT",
       "PATCH",
       "POST",
       "DELETE"
      ],
      "CachePolicyId": "4135ea2d-6df8-44a3-9df3-4b5a84be39ad",
      "Compress": true,
      "OriginRequestPolicyId": "216adef6-5c7f-47e4-b989-5492eafa07d3",
      "TargetOriginId": "fleetworkshopIDEFleetIdeDistributionOrigin1C40470CE",
      "ViewerProtocolPolicy": "allow-all"
     },
     "Enabled": true,
     "HttpVersion": "http2",
     "IPV6Enabled": true,
     "Origins": [
      {
       "CustomOriginConfig": {
        "HTTPPort": 80,
        "OriginProtocolPolicy": "http-only",
        "OriginSSLProtocols": [
         "TLSv1.2"
        ]
       },
       "DomainName": {
        "Fn::GetAtt": [
         "IDEFleet0EC23AB4641a0cee2d30d2eb",
         "PublicDnsName"
        ]
       },
       "Id": "fleetworkshopIDEFleetIdeDistributionOrigin1C40470CE"
      }
     ]
    }
   }
  },
  "IDEFleetIdeBootstrapWaitConditionHandle6F73F067": {
   "Type": "AWS::CloudFormation::WaitConditionHandle"
  },
  "IDEFleetIdeBootstrapWaitConditionBAEC1144": {
   "Type": "AWS::CloudFormation::WaitCondition",
   "Properties": {
    "Count": 1,
    "Handle": {
     "Ref": "IDEFleetIdeBootstrapWaitConditionHandle6F73F067"
    },
    "Timeout": "2700"
   },
   "DependsOn": [
    "IDEFleetInstanceProfile1BF87146",
    "IDEFleet0EC23AB4641a0cee2d30d2eb",
    "IDEFleetIdeBootstrapDocument88102810"
   ]
  },
  "IDEFleetIdeBootstrapDocument88102810": {
   "Type": "AWS::SSM::Document",
   "Properties": {
    "Content": {
     "schemaVersion": "2.2",
     "description": "Bootstrap IDE",
     "parameters": {
      "BootstrapScript": {
       "type": "String",
       "description": "(Optional) Custom bootstrap script to run.",
       "default": ""
      }
     },
     "mainSteps": [
      {
       "action": "aws:runShellScript",
       "name": "IdeBootstrapFunction",
       "inputs": {
        "runCommand": [
         {
          "Fn::Sub": [
           "bash << 'HEREDOC'\nset -e\n\necho \"=== Starting IDE Bootstrap Process ===\"\necho \"Timestamp: $(date)\"\n\necho \"Retrieving IDE password...\"\n\nPASSWORD_SECRET_VALUE=$(aws secretsmanager get-secret-value --secret-id \"${passwordName}\" --query 'SecretString' --output text)\n\nexport IDE_PASSWORD=$(echo \"$PASSWORD_SECRET_VALUE\" | jq -r '.password')\n\necho \"Setting profile variables...\"\n\n# Set some useful variables\nexport TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\nexport AWS_REGION=$(curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep region | awk -F\\\" '{print $4}')\nexport EC2_PRIVATE_IP=$(curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -s http://169.254.169.254/latest/meta-data/local-ipv4)\n\ntee /etc/profile.d/workshop.sh <<EOF\nexport INSTANCE_IAM_ROLE_NAME=\"${instanceIamRoleName}\"\nexport INSTANCE_IAM_ROLE_ARN=\"${instanceIamRoleArn}\"\n\nexport AWS_REGION=\"$AWS_REGION\"\nexport EC2_PRIVATE_IP=\"$EC2_PRIVATE_IP\"\n\nexport IDE_DOMAIN=\"${domain}\"\nexport IDE_URL=\"https://${domain}\"\nexport IDE_PASSWORD=\"$IDE_PASSWORD\"\n\nalias code=\"code-server\"\nEOF\n\nsource /etc/profile.d/workshop.sh\n\n# Create SSM parameter for Gitea External URL (fix for circular dependency)\necho \"Creating GiteaExternalUrl SSM parameter...\"\necho \"IDE_DOMAIN value: $IDE_DOMAIN\"\nif aws ssm put-parameter --type String --name GiteaExternalUrl --value \"https://$IDE_DOMAIN/gitea\" --overwrite; then\n  echo \"Successfully created GiteaExternalUrl SSM parameter\"\nelse\n  echo \"Warning: Failed to create GiteaExternalUrl SSM parameter, but continuing...\"\nfi\n\necho \"Setting PS1...\"\n\n# Set PS1\ntee /etc/profile.d/custom_prompt.sh <<EOF\n#!/bin/sh\n\nexport PROMPT_COMMAND='export PS1=\"\\u:\\w:$ \"'\nEOF\n\necho \"Generating SSH key...\"\n\n# Generate an SSH key for ec2-user\nsudo -u ec2-user bash -c \"ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa -m pem <<< y\"\n\necho \"Installing AWS CLI...\"\n\n# Install AWS CLI\ncurl -LSsf -o /tmp/aws-cli.zip https://awscli.amazonaws.com/awscli-exe-linux-$(uname -m).zip\nunzip -q -d /tmp /tmp/aws-cli.zip\n/tmp/aws/install --update\nrm -rf /tmp/aws\n\necho \"Installing Docker...\"\n\n# Install docker and base package\ndnf install -y -q docker git\nservice docker start\nusermod -aG docker ec2-user\n\necho \"Installing code-server...\"\n\n# Install code-server\ncodeServer=$(dnf list installed code-server | wc -l)\nif [ \"$codeServer\" -eq \"0\" ]; then\n  sudo -u ec2-user \"codeServerVersion=${codeServerVersion}\" bash -c 'curl -fsSL https://code-server.dev/install.sh | sh -s -- --version ${codeServerVersion}'\n  systemctl enable --now code-server@ec2-user\nfi\n\nsudo -u ec2-user bash -c 'mkdir -p ~/.config/code-server'\nsudo -u ec2-user bash -c 'touch ~/.config/code-server/config.yaml'\ntee /home/ec2-user/.config/code-server/config.yaml <<EOF\ncert: false\nauth: password\npassword: \"$IDE_PASSWORD\"\nbind-addr: 127.0.0.1:8889\nEOF\n\n# Create default directory for workspace\nsudo -u ec2-user bash -c 'mkdir -p ~/environment'\n\nENVIRONMENT_CONTENTS_ZIP=${environmentContentsZip}\n\nif [ ! -z \"$ENVIRONMENT_CONTENTS_ZIP\" ]; then\n  echo \"Adding environments archive...\"\n\n  if [[ $ENVIRONMENT_CONTENTS_ZIP == s3:* ]]; then\n    aws s3 cp $ENVIRONMENT_CONTENTS_ZIP /tmp/environment.zip\n  else\n    curl -LSsf -o /tmp/environment.zip $ENVIRONMENT_CONTENTS_ZIP\n  fi\n\n  sudo -u ec2-user bash -c 'unzip -q /tmp/environment.zip -d ~/environment'\n\n  rm -rf /tmp/environment.zip\nfi\n\nSTARTUP_EDITOR='none'\n\nTERMINAL_ON_STARTUP=\"${terminalOnStartup}\"\nREADME_URL=\"${readmeUrl}\"\n\nif [ ! -z \"$README_URL\" ]; then\n  echo \"Adding README...\"\n  if [[ $README_URL == s3:* ]]; then\n    aws s3 cp $README_URL /home/ec2-user/environment/README.md\n  else\n    curl -LSsf -o /home/ec2-user/environment/README.md $README_URL\n  fi\nfi\n\nif [ \"$TERMINAL_ON_STARTUP\" = \"true\" ]; then\n  STARTUP_EDITOR='terminal'\nelif [ -f /home/ec2-user/environment/README.md ]; then\n  STARTUP_EDITOR='readme'\nfi\n\necho \"Configuring code-server...\"\n\nsudo -u ec2-user bash -c 'mkdir -p ~/.local/share/code-server/User'\nsudo -u ec2-user bash -c 'touch ~/.local/share/code-server/User/settings.json'\ntee /home/ec2-user/.local/share/code-server/User/settings.json <<EOF\n{\n  \"extensions.autoUpdate\": false,\n  \"extensions.autoCheckUpdates\": false,\n  \"security.workspace.trust.enabled\": false,\n  \"workbench.startupEditor\": \"$STARTUP_EDITOR\",\n  \"task.allowAutomaticTasks\": \"on\",\n  \"telemetry.telemetryLevel\": \"off\"\n}\nEOF\n\nsudo -u ec2-user bash -c 'touch ~/.local/share/code-server/User/keybindings.json'\ntee /home/ec2-user/.local/share/code-server/User/keybindings.json << 'EOF'\n[\n  {\n    \"key\": \"shift+cmd+/\",\n    \"command\": \"remote.tunnel.forwardCommandPalette\"\n  }\n]\nEOF\n\nif [ ! -z \"${splashUrl}\" ]; then\necho \"Configuring splash URL...\"\n\nsudo -u ec2-user bash -c 'touch ~/.local/share/code-server/User/tasks.json'\ntee /home/ec2-user/.local/share/code-server/User/tasks.json << 'EOF'\n{\n  \"version\": \"2.0.0\",\n  \"tasks\": [\n    {\n      \"label\": \"Open Splash\",\n      \"command\": \"${!input:openSimpleBrowser}\",\n      \"presentation\": {\n        \"reveal\": \"always\",\n        \"panel\": \"new\"\n      },\n      \"runOptions\": {\n        \"runOn\": \"folderOpen\"\n      }\n    }\n  ],\n  \"inputs\": [\n    {\n      \"id\": \"openSimpleBrowser\",\n      \"type\": \"command\",\n      \"command\": \"simpleBrowser.show\",\n      \"args\": [\n        \"${splashUrl}\"\n      ]\n    }\n  ]\n}\nEOF\nfi\n\necho \"Installing code-server extensions...\"\n\nEXTENSIONS=\"${extensions}\"\n\nIFS=',' read -ra array <<< \"$EXTENSIONS\"\n\n# Iterate over each entry in the array\nfor extension in \"${!array[@]}\"; do\n  # Use retries as extension installation seems unreliable\n  sudo -u ec2-user bash -c \"set -e; (r=5;while ! code-server --install-extension $extension --force ; do ((--r))||exit;sleep 5;done)\"\ndone\n\nif [ ! -f \"/home/ec2-user/.local/share/code-server/coder.json\" ]; then\n  sudo -u ec2-user bash -c 'touch ~/.local/share/code-server/coder.json'\n  echo '{ \"query\": { \"folder\": \"/home/ec2-user/environment\" } }' > /home/ec2-user/.local/share/code-server/coder.json\nfi\n\necho \"Restarting code-server...\"\n\nsystemctl restart code-server@ec2-user\n\necho \"Installing Caddy...\"\n\n# Install caddy\ndnf copr enable -y -q @caddy/caddy epel-9-x86_64\ndnf install -y -q caddy\nsystemctl enable --now caddy\n\ntee /etc/caddy/Caddyfile <<EOF\nhttp://${domain} {\n  handle /* {\n    reverse_proxy 127.0.0.1:8889\n  }\n  #GITEA\n}\nEOF\n\necho \"Restarting caddy...\"\n\nsystemctl restart caddy\n\nif [ ! -f \"/home/ec2-user/.local/share/code-server/coder.json\" ]; then\n  sudo -u ec2-user bash -c 'touch ~/.local/share/code-server/coder.json'\n  echo '{ \"query\": { \"folder\": \"/home/ec2-user/environment\" } }' > /home/ec2-user/.local/share/code-server/coder.json\nfi\n\n${installGitea}\n\necho \"Running custom bootstrap script...\"\n\n${customBootstrapScript}\nHEREDOC\n\nexit_code=$?\n\n/opt/aws/bin/cfn-signal -e $exit_code '${waitConditionHandleUrl}'\n\nexit $exit_code",
           {
            "instanceIamRoleName": {
             "Ref": "SharedRoleD1D02F7E"
            },
            "instanceIamRoleArn": {
             "Fn::GetAtt": [
              "SharedRoleD1D02F7E",
              "Arn"
             ]
            },
            "passwordName": {
             "Fn::Join": [
              "-",
              [
               {
                "Fn::Select": [
                 0,
                 {
                  "Fn::Split": [
                   "-",
                   {
                    "Fn::Select": [
                     6,
                     {
                      "Fn::Split": [
                       ":",
                       {
                        "Ref": "IDEFleetIdePasswordSecret4C89AB01"
                       }
                      ]
                     }
                    ]
                   }
                  ]
                 }
                ]
               },
               {
                "Fn::Select": [
                 1,
                 {
                  "Fn::Split": [
                   "-",
                   {
                    "Fn::Select": [
                     6,
                     {
                      "Fn::Split": [
                       ":",
                       {
                        "Ref": "IDEFleetIdePasswordSecret4C89AB01"
                       }
                      ]
                     }
                    ]
                   }
                  ]
                 }
                ]
               }
              ]
             ]
            },
            "domain": {
             "Fn::GetAtt": [
              "IDEFleetIdeDistribution631906CF",
              "DomainName"
             ]
            },
            "codeServerVersion": "4.93.1",
            "waitConditionHandleUrl": {
             "Ref": "IDEFleetIdeBootstrapWaitConditionHandle6F73F067"
            },
            "customBootstrapScript": {
             "Fn::Sub": [
              "#!/bin/bash\nset -x\nsudo sh -c \"echo LANG=en_US.utf-8 >> /etc/environment\"\nsudo sh -c \"echo LC_ALL=en_US.UTF-8 >> /etc/environment\"\n# . /home/ec2-user/.bashrc\nsudo yum -y install sqlite telnet jq strace tree gcc glibc-static python3 python3-pip gettext bash-completion npm zsh util-linux-user locate\necho '=== INSTALL and CONFIGURE default software components ==='\n\naws configure set cli_pager \"\"\n\nexport TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\nexport AWS_REGION=$(curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep region | awk -F\\\" '{print $4}')\nexport ACCOUNTID=$(aws sts get-caller-identity | jq -r .Account)\nexport AWS_ACCOUNT_ID=$ACCOUNTID\nexport ACCOUNT_ID=$ACCOUNTID\nexport ASSETS_BUCKET_NAME=${AssetsBucketName} # Coming from Fn.Sub\nexport ASSETS_BUCKET_PREFIX=${AssetsBucketPrefix} # Coming from Fn.Sub\nexport BUCKET_NAME=${BUCKET_NAME} # Coming from Fn.Sub\nexport WORKSHOP_GIT_URL=${WORKSHOP_GIT_URL} # Coming from Fn.Sub\nexport WORKSHOP_GIT_BRANCH=${WORKSHOP_GIT_BRANCH} # Coming from Fn.Sub\nexport BASE_DIR=/home/ec2-user/environment/fleet-management-on-amazon-eks-workshop\nexport GITOPS_DIR=/home/ec2-user/environment/gitops-repos\nexport GOROOT=/usr/local/go\n\n# Enhanced error handling for bootstrap process\nset +e  # Disable exit on error for this section\necho \"=== Starting Custom Bootstrap Script ===\"\necho \"Timestamp: $(date)\"\n\nsudo bash -c \"cat > /usr/local/bin/wait-for-lb\" <<'EOT'\n#!/bin/bash\nset -e\nexport host=$1\n\nif [ -z \"$host\" ]; then\necho \"the service is not found: $host\"\nexit\nfi\n\necho $host\n\nset -Eeuo pipefail\n\necho \"Waiting for $host...\"\n\nEXIT_CODE=0\n\ntimeout -s TERM 600 bash -c \\\n'while [[ \"$(curl -s -o /dev/null -L -w ''%{http_code}'' http://$host/home)\" != \"200\" ]];\\\ndo sleep 5;\\\ndone' || EXIT_CODE=$?\n\nif [ $EXIT_CODE -ne 0 ]; then\necho \"Load balancer did not become available or return HTTP 200 for 600 seconds\"\nexit 1\nfi\n\necho \"You can now access http://$host\"\nEOT\nsudo chmod 755 /usr/local/bin/wait-for-lb\nsudo bash -c \"cat > /usr/local/bin/wait-for-lb-argocd\" <<'EOT'\n#!/bin/bash\nset -e\nexport host=$1\n\nif [ -z \"$host\" ]; then\necho \"the service is not found: $host\"\nexit\nfi\n\necho $host\n\nset -Eeuo pipefail\n\necho \"Waiting for $host...\"\n\nEXIT_CODE=0\n\ntimeout -s TERM 600 bash -c \\\n'while [[ \"$(curl -s -k -o /dev/null -L -w ''%{http_code}'' http://$host/)\" != \"200\" ]];\\\ndo sleep 5;\\\ndone' || EXIT_CODE=$?\n\nif [ $EXIT_CODE -ne 0 ]; then\necho \"Load balancer did not become available or return HTTP 200 for 600 seconds\"\nexit 1\nfi\n\necho \"You can now access http://$host\"\nEOT\nsudo chmod 755 /usr/local/bin/wait-for-lb-argocd\n\nsudo curl --silent --location -o /usr/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\nsudo chmod +x /usr/bin/kubectl\n\nsudo curl --silent --location -o /usr/bin/argocd https://github.com/argoproj/argo-cd/releases/download/v2.12.2/argocd-linux-amd64\nsudo chmod +x /usr/bin/argocd\n\ncurl --silent --location \"https://get.helm.sh/helm-v3.10.1-linux-amd64.tar.gz\" | tar xz -C /tmp\nsudo mv -f /tmp/linux-amd64/helm /usr/bin\nsudo chmod +x /usr/bin/helm\n\nsudo curl --silent --location -o /tmp/terraform.zip \"https://releases.hashicorp.com/terraform/1.9.3/terraform_1.9.3_linux_amd64.zip\"\ncd /tmp && unzip -o /tmp/terraform.zip && cd -\nchmod +x /tmp/terraform\nsudo mv -f /tmp/terraform /usr/bin\n\nsudo curl --silent --location \"https://go.dev/dl/go1.23.1.linux-amd64.tar.gz\" | sudo tar xz -C /usr/local\n\n\nsudo su - ec2-user <<EOF\nset -x\nexport | sort\n\naws configure set cli_pager \"\"\n\n# start of cloud9-init script\nkubectl completion bash >>  ~/.bash_completion\nargocd completion bash >>  ~/.bash_completion\nhelm completion bash >>  ~/.bash_completion\necho \"alias k=kubectl\" >> ~/.bashrc\necho \"alias kgn='kubectl get nodes -L beta.kubernetes.io/arch -L eks.amazonaws.com/capacityType -L beta.kubernetes.io/instance-type -L eks.amazonaws.com/nodegroup -L topology.kubernetes.io/zone -L karpenter.sh/provisioner-name -L karpenter.sh/capacity-type'\" | tee -a ~/.bashrc\necho \"alias ll='ls -la'\" >> ~/.bashrc\n\necho \"alias ktx=kubectx\" >> ~/.bashrc\necho \"alias kctx=kubectx\" >> ~/.bashrc\necho \"alias kns=kubens\" >> ~/.bashrc\necho \"export TERM=xterm-color\" >> ~/.bashrc\n\necho \"alias code=/usr/lib/code-server/bin/code-server\" >> ~/.bashrc\necho \"complete -F __start_kubectl k\" >> ~/.bashrc\ncurl -sS https://webinstall.dev/k9s | bash\n\n#Install Krew and stern\n(\n  cd \\$(mktemp -d) && pwd &&\n  OS=\\$(uname | tr '[:upper:]' '[:lower:]') &&\n  ARCH=\\$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/') &&\n  KREW=krew-\\${!OS}_\\${!ARCH} && echo \\$KREW\n  curl -fsSLO https://github.com/kubernetes-sigs/krew/releases/latest/download/\\${!KREW}.tar.gz &&\n  tar zxvf \\${!KREW}.tar.gz &&\n  ./\\${!KREW} install krew\n)\necho \"export PATH=${!KREW_ROOT:-/home/ec2-user/.krew}/bin:/home/ec2-user/.local/bin:/usr/local/go/bin:~/go/bin:$PATH\" | tee -a ~/.bashrc\nexport PATH=${!KREW_ROOT:-/home/ec2-user/.krew}/bin:/home/ec2-user/.local/bin:/usr/local/go/bin:~/go/bin:$PATH\nkubectl krew install stern\nkubectl krew install np-viewer \n\ngo install github.com/kyverno/chainsaw@latest\npip install pytest\npip install pytest_bdd boto3 kubernetes\n\ncurl -sfL https://direnv.net/install.sh | bash\n\n#Try Install some VsCode plugins\n/usr/lib/code-server/bin/code-server --install-extension hashicorp.terraform || true\n/usr/lib/code-server/bin/code-server --install-extension moshfeu.compare-folders || true\n/usr/lib/code-server/bin/code-server --install-extension amazonwebservices.amazon-q-vscode || true\n\n#Install Amazon Q\ncurl --proto '=https' --tlsv1.2 -sSf \"https://desktop-release.q.us-east-1.amazonaws.com/latest/q-x86_64-linux.zip\" -o \"/tmp/q.zip\"\nunzip /tmp/q.zip -d /tmp\n/tmp/q/install.sh --no-confirm\n\n#Install ag silver search\n## Install build dependencies\nsudo dnf install -y git gcc make pkg-config automake autoconf pcre-devel xz-devel zlib-devel\n## Clone the repository, build and install from source\ncd /tmp && git clone https://github.com/ggreer/the_silver_searcher.git && \\\ncd the_silver_searcher && \\\n./build.sh && \\\nsudo make install\n\n#Install fuzzy search\ngit clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf\n~/.fzf/install --all  \n\n#Install zsh\nsudo -k chsh -s /bin/zsh ec2-user\njq '. + {\"terminal.integrated.defaultProfile.linux\": \"zsh\"}' /home/ec2-user/.local/share/code-server/User/settings.json > temp.json && mv temp.json /home/ec2-user/.local/share/code-server/User/settings.json\nrm -rf ~/.oh-my-zsh\n\nwget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\nCHSH=no RUNZSH=no sh install.sh\n\ngit clone https://github.com/romkatv/powerlevel10k.git ~/.oh-my-zsh/custom/themes/powerlevel10k\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\ngit clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions\ngit clone https://github.com/zsh-users/zsh-history-substring-search ~/.oh-my-zsh/custom/plugins/zsh-history-substring-search\n\n\n#Install workshop\nmkdir -p $BASE_DIR\ngit clone $WORKSHOP_GIT_URL $BASE_DIR\ncd $BASE_DIR\ngit checkout $WORKSHOP_GIT_BRANCH\n\ncp hack/.zshrc hack/.p10k.zsh ~/\n\n# Setup bashrc\nls -lt ~\nmkdir -p ~/.bashrc.d\ncp $BASE_DIR/hack/.bashrc.d/* ~/.bashrc.d/\n\n# Common backend config\ncat << EOT > $BASE_DIR/terraform/common/backend_override.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"$BUCKET_NAME\"\n    key            = \"common/terraform.tfstate\"\n    region         = \"$AWS_REGION\"\n  }\n}\nEOT\n\n\n# Hub backend config\ncat << EOT > $BASE_DIR/terraform/hub/backend_override.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"$BUCKET_NAME\"\n    key            = \"hub/terraform.tfstate\"\n    region         = \"$AWS_REGION\"\n  }\n}\nEOT\n\n\n# Spokes backend config\ncat << EOT > $BASE_DIR/terraform/spokes/backend_override.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"$BUCKET_NAME\"\n    key            = \"spokes/terraform.tfstate\"\n    region         = \"$AWS_REGION\"\n    workspace_key_prefix = \"spokes\"\n  }\n}\nEOT\n\n\nEOF\n\n#install kubectx & kubens\nsudo rm -rf /opt/kubectx\nsudo git clone https://github.com/ahmetb/kubectx /opt/kubectx\nsudo ln -sf /opt/kubectx/kubectx /usr/local/bin/kubectx\nsudo ln -sf /opt/kubectx/kubens /usr/local/bin/kubens\n\n\nsudo curl -L https://github.com/awslabs/eks-node-viewer/releases/download/v0.7.1/eks-node-viewer_Linux_x86_64 -o /usr/local/bin/eks-node-viewer  && sudo chmod +x $_\n\nsource ~/.bashrc\n\n# end of cloud9-init script\n\necho '=== Configure .bashrc.d ==='\nif [[ ! -d \"/home/ec2-user/.bashrc.d\" ]]; then\n    sudo -H -u ec2-user bash -c \"mkdir -p ~/.bashrc.d\"\n    cat << EOT > /home/ec2-user/.bashrc.d/env.bash\n    export ACCOUNTID=$ACCOUNTID\n    export ACCOUNT_ID=$ACCOUNTID\n    export AWS_ACCOUNT_ID=$ACCOUNTID\n    export AWS_DEFAULT_REGION=$AWS_REGION\n    export GOROOT=/usr/local/go\nEOT\n\n    sudo -H -u ec2-user bash -c \"cat <<'EOF' >> ~/.bashrc \nfor file in ~/.bashrc.d/*.bash; do\n  source \"\\$file\" || true\ndone\nEOF\n\"\n\nfi\n\n# # change permissions for /root/.ssh for the codecommit TF step to be able to change the file location from /root to /home/ec2-user\n# chown -R ec2-user:ec2-user /root/.ssh\n# chown -R ec2-user:ec2-user /root\n# chmod 700 /root/.ssh\n# chmod 700 /root\n\necho '=== CONFIGURE awscli and setting ENVIRONMENT VARS ==='\necho \"complete -C '/usr/local/bin/aws_completer' aws\" >> /home/ec2-user/.bashrc.d/aws.bash\necho '=== Run init script ==='\n# aws s3 cp s3://${AssetsBucketName}/${AssetsBucketPrefix}cloud9-init.sh /tmp && bash /tmp/cloud9-init.sh\necho '=== CLEANING /home/ec2-user ==='\n# for f in cloud9; do rm -rf /home/ec2-user/$f; done # cloud9 doesn't exists\nchown -R ec2-user:ec2-user /home/ec2-user/\n#Don't reboot in ssm document, that break the execution\necho \"Bootstrap completed with return code $?\"",
              {
               "BUCKET_NAME": {
                "Ref": "TFStateBackendBucketF0FC9A9D"
               },
               "AssetsBucketName": {
                "Fn::FindInMap": [
                 "WorkshopAssetsBucketAssetsStaticBucketMapping",
                 {
                  "Ref": "AWS::Region"
                 },
                 "bucketName"
                ]
               },
               "AssetsBucketPrefix": "28c283c1-1d60-43fa-a604-4e983e0e8038/",
               "WORKSHOP_GIT_URL": "https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop",
               "WORKSHOP_GIT_BRANCH": "v1.0.2"
              }
             ]
            },
            "installGitea": "dnf install -y nerdctl cni-plugins\nmkdir -p /gitea/config /gitea/data\n\necho '\nversion: \"2\"\n\nservices:\n  gitea:\n    image: gitea/gitea:1.22-rootless\n    restart: always\n    volumes:\n      - /gitea/data:/var/lib/gitea\n      - /gitea/config:/etc/gitea\n      - /etc/timezone:/etc/timezone:ro\n      - /etc/localtime:/etc/localtime:ro\n    ports:\n      - \"9999:3000\"\n      - \"2222:2222\"\n' > gitea.yaml\n\necho \"\nAPP_NAME = Gitea: Git with a cup of tea\nRUN_MODE = prod\nRUN_USER = git\nWORK_PATH = /var/lib/gitea\n\n[repository]\nROOT = /var/lib/gitea/git/repositories\nENABLE_PUSH_CREATE_USER = true\nDISABLE_HTTP_GIT = false\n\n[repository.local]\nLOCAL_COPY_PATH = /var/lib/gitea/tmp/local-repo\n\n[repository.upload]\nTEMP_PATH = /var/lib/gitea/uploads\n\n[server]\nAPP_DATA_PATH = /var/lib/gitea\nDOMAIN = $EC2_PRIVATE_IP\nSSH_DOMAIN = $EC2_PRIVATE_IP\nSSH_CREATE_AUTHORIZED_KEYS_FILE=false\nHTTP_PORT = 3000\nROOT_URL = http://$EC2_PRIVATE_IP:9000/gitea\nDISABLE_SSH = false\nSSH_PORT = 2222\nSSH_LISTEN_PORT = 2222\nSTART_SSH_SERVER = true\nLFS_START_SERVER = true\nOFFLINE_MODE = true\n\n[database]\nPATH = /var/lib/gitea/gitea.db\nDB_TYPE = sqlite3\nHOST = localhost:3306\nNAME = gitea\nUSER = root\nPASSWD = \nLOG_SQL = false\nSCHEMA = \nSSL_MODE = disable\n\n[indexer]\nISSUE_INDEXER_PATH = /var/lib/gitea/indexers/issues.bleve\n\n[session]\nPROVIDER_CONFIG = /var/lib/gitea/sessions\nPROVIDER = file\n\n[picture]\nAVATAR_UPLOAD_PATH = /var/lib/gitea/avatars\nREPOSITORY_AVATAR_UPLOAD_PATH = /var/lib/gitea/repo-avatars\n\n[attachment]\nPATH = /var/lib/gitea/attachments\n\n[log]\nMODE = console\nLEVEL = info\nROOT_PATH = /var/lib/gitea/log\n\n[security]\nINSTALL_LOCK = true\nSECRET_KEY = \nREVERSE_PROXY_LIMIT = 1\nREVERSE_PROXY_TRUSTED_PROXIES = *\nPASSWORD_HASH_ALGO = pbkdf2\n\n[service]\nDISABLE_REGISTRATION = true\nREQUIRE_SIGNIN_VIEW = true\nREGISTER_EMAIL_CONFIRM = false\nENABLE_NOTIFY_MAIL = false\nALLOW_ONLY_EXTERNAL_REGISTRATION = false\nENABLE_CAPTCHA = false\nDEFAULT_KEEP_EMAIL_PRIVATE = false\nDEFAULT_ALLOW_CREATE_ORGANIZATION = true\nDEFAULT_ENABLE_TIMETRACKING = true\nNO_REPLY_ADDRESS = noreply.localhost\n\n[lfs]\nPATH = /var/lib/gitea/git/lfs\n\n[mailer]\nENABLED = false\n\n[cron.update_checker]\nENABLED = false\n\n[repository.pull-request]\nDEFAULT_MERGE_STYLE = merge\n\n[repository.signing]\nDEFAULT_TRUST_MODEL = committer\n\n\" > /gitea/config/app.ini\nchown -R 1000:1000 /gitea\nsudo nerdctl compose -f gitea.yaml up -d --quiet-pull\n\n# We need to be idempotent and check for locked database\nwhile true; do\n    CONTAINER=$(sudo nerdctl compose -f gitea.yaml ps --format json | jq .[].Name) # Name is <folder>-<compose-name>-1\n\n    if [ ! -z \"$CONTAINER\" ]; then\n      STATUS=$(sudo nerdctl exec $CONTAINER -- sh -c \"gitea admin user create --username workshop-user --email workshop-user@example.com --password $IDE_PASSWORD 2>&1 || exit 0\")\n      [[ \"$STATUS\" =~ .*locked|no\\ such\\ table.* ]] || break\n    fi\n    sleep 5;\ndone\n\ntee -a /etc/caddy/Caddyfile <<EOF\nhttp://$IDE_DOMAIN:9000, http://localhost:9000 {\n  handle_path /proxy/9000/* {\n    reverse_proxy 127.0.0.1:9999\n  }\n\n  handle /* {\n    reverse_proxy 127.0.0.1:9999\n  }\n}\nEOF\n\n# We add the handle_path in the cloudfront site\nsed -i 's~#GITEA~handle_path /gitea/* { \\\n    reverse_proxy 127.0.0.1:9999 \\\n  }~' /etc/caddy/Caddyfile\n\nsystemctl restart caddy\n\nsleep 5\n\nsudo -u ec2-user bash -c 'git config --global user.email \"workshop-user@example.com\"'\nsudo -u ec2-user bash -c 'git config --global user.name \"Workshop User\"'\n\nsudo -u ec2-user bash -c 'touch ~/.ssh/config'\ntee /home/ec2-user/.ssh/config <<EOF\nHost $EC2_PRIVATE_IP\n  User git\n  Port 2222\n  IdentityFile /home/ec2-user/.ssh/id_rsa\n  IdentitiesOnly yes\nEOF\n\nsudo -u ec2-user bash -c 'chmod 600 ~/.ssh/*'\n\nPUB_KEY=$(sudo cat /home/ec2-user/.ssh/id_rsa.pub)\nTITLE=\"$(hostname)$(date +%s)\"\n\ncurl -X 'POST' \\\n  \"http://workshop-user:$IDE_PASSWORD@localhost:9000/api/v1/user/keys\" \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d \"{\n  \\\"key\\\": \\\"$PUB_KEY\\\",\n  \\\"read_only\\\": true,\n  \\\"title\\\": \\\"$TITLE\\\"\n}\"\n\ntee /etc/profile.d/gitea.sh <<EOF\nexport GIT_SSH_ENDPOINT=\"$EC2_PRIVATE_IP:2222\"\nexport GITEA_API_ENDPOINT=\"http://$EC2_PRIVATE_IP:9000\"\nexport GITEA_EXTERNAL_URL=\"https://$IDE_DOMAIN/gitea\"\nexport GITEA_PASSWORD=\"$IDE_PASSWORD\"\nEOF\n\nsource /etc/profile.d/gitea.sh\n",
            "splashUrl": "",
            "readmeUrl": "",
            "environmentContentsZip": "",
            "extensions": "",
            "terminalOnStartup": "false"
           }
          ]
         }
        ]
       }
      }
     ]
    },
    "DocumentFormat": "YAML",
    "DocumentType": "Command",
    "UpdateMethod": "NewVersion"
   }
  },
  "IDEFleetIdeBootstrapResource98864E55": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "IDEFleetIdeBootstrapFunctionC79D571D",
      "Arn"
     ]
    },
    "InstanceId": {
     "Ref": "IDEFleet0EC23AB4641a0cee2d30d2eb"
    },
    "SsmDocument": {
     "Ref": "IDEFleetIdeBootstrapDocument88102810"
    },
    "LogGroupName": {
     "Ref": "IDEFleetIdeLogGroup63888DED"
    }
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "IDEFleetIdePasswordExporterFunctionServiceRoleCC2D9ED3": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "IDEFleetIdePasswordExporterFunctionServiceRoleDefaultPolicy936E9BE2": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "secretsmanager:DescribeSecret",
        "secretsmanager:GetSecretValue"
       ],
       "Effect": "Allow",
       "Resource": {
        "Ref": "IDEFleetIdePasswordSecret4C89AB01"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "IDEFleetIdePasswordExporterFunctionServiceRoleDefaultPolicy936E9BE2",
    "Roles": [
     {
      "Ref": "IDEFleetIdePasswordExporterFunctionServiceRoleCC2D9ED3"
     }
    ]
   }
  },
  "IDEFleetIdePasswordExporterFunction732E769D": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "import traceback\nimport cfnresponse\nimport boto3\nimport json\n\ndef lambda_handler(event, context):\n    print('Event: {}'.format(event))\n    print('context: {}'.format(context))\n    responseData = {}\n\n    status = cfnresponse.SUCCESS\n\n    if event['RequestType'] == 'Delete':\n        cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')\n    else:\n        try:\n            passwordName = event['ResourceProperties']['PasswordName']\n\n            secretsmanager = boto3.client('secretsmanager')\n\n            response = secretsmanager.get_secret_value(\n                SecretId=passwordName,\n            )\n            \n            responseData = json.loads(response['SecretString'])\n        except Exception as e:\n            status = cfnresponse.FAILED\n            tb_err = traceback.format_exc()\n            print(tb_err)\n            responseData = {'Error': tb_err}\n        finally:\n            cfnresponse.send(event, context, status, responseData, 'CustomResourcePhysicalID')"
    },
    "Handler": "index.lambda_handler",
    "Role": {
     "Fn::GetAtt": [
      "IDEFleetIdePasswordExporterFunctionServiceRoleCC2D9ED3",
      "Arn"
     ]
    },
    "Runtime": "python3.12",
    "Timeout": 180
   },
   "DependsOn": [
    "IDEFleetIdePasswordExporterFunctionServiceRoleDefaultPolicy936E9BE2",
    "IDEFleetIdePasswordExporterFunctionServiceRoleCC2D9ED3"
   ]
  },
  "IDEFleetIdePasswordExporterF99DE17D": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "IDEFleetIdePasswordExporterFunction732E769D",
      "Arn"
     ]
    },
    "PasswordName": {
     "Fn::Join": [
      "-",
      [
       {
        "Fn::Select": [
         0,
         {
          "Fn::Split": [
           "-",
           {
            "Fn::Select": [
             6,
             {
              "Fn::Split": [
               ":",
               {
                "Ref": "IDEFleetIdePasswordSecret4C89AB01"
               }
              ]
             }
            ]
           }
          ]
         }
        ]
       },
       {
        "Fn::Select": [
         1,
         {
          "Fn::Split": [
           "-",
           {
            "Fn::Select": [
             6,
             {
              "Fn::Split": [
               ":",
               {
                "Ref": "IDEFleetIdePasswordSecret4C89AB01"
               }
              ]
             }
            ]
           }
          ]
         }
        ]
       }
      ]
     ]
    }
   },
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "EKSGITIAMStackDeployProjectE1461187": {
   "Type": "AWS::CodeBuild::Project",
   "Properties": {
    "Artifacts": {
     "Type": "NO_ARTIFACTS"
    },
    "Cache": {
     "Type": "NO_CACHE"
    },
    "EncryptionKey": "alias/aws/s3",
    "Environment": {
     "ComputeType": "BUILD_GENERAL1_SMALL",
     "EnvironmentVariables": [
      {
       "Name": "TFSTATE_BUCKET_NAME",
       "Type": "PLAINTEXT",
       "Value": {
        "Ref": "TFStateBackendBucketF0FC9A9D"
       }
      },
      {
       "Name": "WORKSHOP_GIT_URL",
       "Type": "PLAINTEXT",
       "Value": "https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop"
      },
      {
       "Name": "WORKSHOP_GIT_BRANCH",
       "Type": "PLAINTEXT",
       "Value": "v1.0.2"
      },
      {
       "Name": "FORCE_DELETE_VPC",
       "Type": "PLAINTEXT",
       "Value": "true"
      },
      {
       "Name": "GITEA_PASSWORD",
       "Type": "PLAINTEXT",
       "Value": {
        "Fn::GetAtt": [
         "IDEFleetIdePasswordExporterF99DE17D",
         "password"
        ]
       }
      },
      {
       "Name": "IS_WS",
       "Type": "PLAINTEXT",
       "Value": "false"
      }
     ],
     "Image": "aws/codebuild/amazonlinux2-x86_64-standard:4.0",
     "ImagePullCredentialsType": "CODEBUILD",
     "PrivilegedMode": false,
     "Type": "LINUX_CONTAINER"
    },
    "ServiceRole": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "Source": {
     "BuildSpec": "version: 0.2\nphases:\n  pre_build:\n    commands:\n      - |\n        yum install -y gettext\n        # Helm installation\n        curl --silent --location \"https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz\" | tar xz -C /tmp\n        mv /tmp/linux-amd64/helm /usr/local/bin\n        chmod +x /usr/local/bin/helm\n        # Terraform installation\n        sudo yum install -y yum-utils\n        sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\n        sudo yum -y install terraform\n  build:\n    commands:\n      - |\n        set -x\n        set -e\n\n        aws configure set cli_pager \"\"\n\n        ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n        BUCKET_NAME=${TFSTATE_BUCKET_NAME}\n        BASE_DIR=${CODEBUILD_SRC_DIR}\n        WORKSHOP_GIT_URL=${WORKSHOP_GIT_URL:-https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop}\n        WORKSHOP_GIT_BRANCH=${WORKSHOP_GIT_BRANCH:-v1.0.2}\n        IS_WS=${IS_WS:-false}\n        GITEA_PASSWORD=${GITEA_PASSWORD}\n\n        env | sort\n\n        # clone and create CodeCommit repo\n        git clone $WORKSHOP_GIT_URL $BASE_DIR || true\n        cd $BASE_DIR\n        git checkout $WORKSHOP_GIT_BRANCH\n        cd -\n\n        aws ssm put-parameter --name \"tf-backend-bucket\" --type \"String\" --value \"$BUCKET_NAME\" --overwrite\n\n        # This gives access to the EKS cluster in terraform\n\n        # Common backend config\n        cat << EOT > $BASE_DIR/terraform/common/backend_override.tf\n        terraform {\n          backend \"s3\" {\n            bucket         = \"$BUCKET_NAME\"\n            key            = \"common/terraform.tfstate\"\n            region         = \"$AWS_REGION\"\n          }\n        }\n        EOT\n\n        # Fix terraform provider version conflicts\n        echo \"Fixing terraform provider version conflicts...\"\n        rm -f $BASE_DIR/terraform/common/.terraform.lock.hcl\n        rm -rf $BASE_DIR/terraform/common/.terraform\n        \n        # Force AWS provider to use compatible version\n        cat > $BASE_DIR/terraform/common/provider_override.tf << 'EOF'\n        terraform {\n          required_providers {\n            aws = {\n              source  = \"hashicorp/aws\"\n              version = \"= 5.95.0\"\n            }\n          }\n        }\n        EOF\n        \n        # Fix terraform init --upgrade in scripts to prevent provider version conflicts\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/common/deploy.sh\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/common/destroy.sh\n\n        mkdir -p ~/.ssh\n\n        # Disable Security Hub resources to prevent subscription errors\n        if [ -f \"$BASE_DIR/terraform/common/securityhub.tf\" ]; then\n          mv \"$BASE_DIR/terraform/common/securityhub.tf\" \"$BASE_DIR/terraform/common/securityhub.tf.disabled\"\n          echo \"Security Hub resources disabled to prevent subscription errors\"\n        fi\n\n        if [[ $REQUESTED_ACTION == 'Delete' ]]; then\n          # Cleanup scripts\n          # Initialize terraform first since we cleaned up lock files\n          cd $BASE_DIR/terraform/common\n          terraform init\n          cd -\n          DEBUG=1 TF_VAR_gitea_external_url=$GITEA_EXTERNAL_URL TF_VAR_gitea_password=$GITEA_PASSWORD $BASE_DIR/terraform/common/destroy.sh\n\n        else\n          # Project Creation\n\n          if [[ $IS_WS == \"true\" ]]; then\n            #Enable Security Hub\n            aws securityhub enable-security-hub || true\n            \n            #Enable QuickSight subscription- required for EKS Fleet Dashboards\n            QUICKSIGHT_ACCOUNT_NAME=\"eks-fleet-workshop\"\n            QS_NOTIFICATION_EMAIL=\"eks-fleet-workshop@amazon.com\"\n            FULL_QS_ACCOUNT_NAME=\"${QUICKSIGHT_ACCOUNT_NAME}-${ACCOUNT_ID}\"\n            \n            aws quicksight create-account-subscription \\\n              --aws-account-id \"$ACCOUNT_ID\" \\\n              --edition ENTERPRISE \\\n              --authentication-method 'IAM_AND_QUICKSIGHT' \\\n              --account-name \"$FULL_QS_ACCOUNT_NAME\" \\\n              --notification-email \"$QS_NOTIFICATION_EMAIL\" || true\n          fi\n\n          GITEA_EXTERNAL_URL=$(aws ssm get-parameter --name \"GiteaExternalUrl\" --query \"Parameter.Value\" --output text || true)\n          for i in $(seq 1 60);\n          do\n            if [[ -z $GITEA_EXTERNAL_URL ]]; then\n              sleep 10\n              GITEA_EXTERNAL_URL=$(aws ssm get-parameter --name \"GiteaExternalUrl\" --query \"Parameter.Value\" --output text || true)\n              echo $GITEA_EXTERNAL_URL\n            else\n              break\n            fi\n          done\n\n          DEBUG=1 TF_VAR_gitea_external_url=$GITEA_EXTERNAL_URL TF_VAR_gitea_password=$GITEA_PASSWORD $BASE_DIR/terraform/common/deploy.sh\n        fi\n  post_build:\n    commands:\n      - echo \">>> build status $CODEBUILD_BUILD_SUCCEEDING \"\n",
     "Type": "NO_SOURCE"
    },
    "TimeoutInMinutes": 90
   }
  },
  "EKSGITIAMStartBuildFunctionServiceRoleC469259C": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSGITIAMStartBuildFunctionServiceRoleDefaultPolicyFCEAE192": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "codebuild:StartBuild",
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSGITIAMStackDeployProjectE1461187",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSGITIAMStartBuildFunctionServiceRoleDefaultPolicyFCEAE192",
    "Roles": [
     {
      "Ref": "EKSGITIAMStartBuildFunctionServiceRoleC469259C"
     }
    ]
   }
  },
  "EKSGITIAMStartBuildFunction2A4FA591": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n  try {\n    const projectName = event.ResourceProperties.ProjectName;\n    const codeBuildIamRoleArn = event.ResourceProperties.CodeBuildIamRoleArn\n\n    const codebuild = new AWS.CodeBuild();\n\n    console.log(`Starting new build of project ${projectName}`);\n\n    const { build } = await codebuild.startBuild({\n      projectName,\n      // Pass CFN related parameters through the build for extraction by the\n      // completion handler.\n      environmentVariablesOverride: [\n        {\n          name: 'CFN_RESPONSE_URL',\n          value: event.ResponseURL\n        },\n        {\n          name: 'CFN_STACK_ID',\n          value: event.StackId\n        },\n        {\n          name: 'CFN_REQUEST_ID',\n          value: event.RequestId\n        },\n        {\n          name: 'CFN_LOGICAL_RESOURCE_ID',\n          value: event.LogicalResourceId\n        },\n        {\n          name: 'REQUESTED_ACTION',\n          value: event.RequestType\n        },\n        {\n          name: 'RESOURCE_CODEBUILD_ROLE_ARN',\n          value: codeBuildIamRoleArn\n        }\n      ]\n    }).promise();\n    console.log(`Build id ${build.id} started - resource completion handled by EventBridge`);\n  } catch(error) {\n    console.error(error);\n    await respond(event, context, 'FAILED', { Error: error });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSGITIAMStartBuildFunctionServiceRoleC469259C",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSGITIAMStartBuildFunctionServiceRoleDefaultPolicyFCEAE192",
    "EKSGITIAMStartBuildFunctionServiceRoleC469259C"
   ]
  },
  "EKSGITIAMReportBuildFunctionServiceRole2D8C0032": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSGITIAMReportBuildFunctionServiceRoleDefaultPolicy60A9BBDC": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "codebuild:BatchGetBuilds",
        "codebuild:ListBuildsForProject"
       ],
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSGITIAMStackDeployProjectE1461187",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSGITIAMReportBuildFunctionServiceRoleDefaultPolicy60A9BBDC",
    "Roles": [
     {
      "Ref": "EKSGITIAMReportBuildFunctionServiceRole2D8C0032"
     }
    ]
   }
  },
  "EKSGITIAMReportBuildFunction6A136575": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n\n  const projectName = event['detail']['project-name'];\n\n  const codebuild = new AWS.CodeBuild();\n\n  const buildId = event['detail']['build-id'];\n  const { builds } = await codebuild.batchGetBuilds({\n    ids: [ buildId ]\n  }).promise();\n\n  console.log(JSON.stringify(builds, null, 4));\n\n  const build = builds[0];\n  // Fetch the CFN resource and response parameters from the build environment.\n  const environment = {};\n  build.environment.environmentVariables.forEach(e => environment[e.name] = e.value);\n\n  const response = {\n    ResponseURL: environment.CFN_RESPONSE_URL,\n    StackId: environment.CFN_STACK_ID,\n    LogicalResourceId: environment.CFN_LOGICAL_RESOURCE_ID,\n    RequestId: environment.CFN_REQUEST_ID\n  };\n\n  if (event['detail']['build-status'] === 'SUCCEEDED') {\n    await respond(response, context, 'SUCCESS', {}, 'build');\n  } else {\n    await respond(response, context, 'FAILED', { Error: 'Build failed' });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSGITIAMReportBuildFunctionServiceRole2D8C0032",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSGITIAMReportBuildFunctionServiceRoleDefaultPolicy60A9BBDC",
    "EKSGITIAMReportBuildFunctionServiceRole2D8C0032"
   ]
  },
  "EKSGITIAMBuildCompleteRule67653565": {
   "Type": "AWS::Events::Rule",
   "Properties": {
    "Description": "Build complete",
    "EventPattern": {
     "source": [
      "aws.codebuild"
     ],
     "detail-type": [
      "CodeBuild Build State Change"
     ],
     "detail": {
      "build-status": [
       "SUCCEEDED",
       "FAILED",
       "STOPPED"
      ],
      "project-name": [
       {
        "Ref": "EKSGITIAMStackDeployProjectE1461187"
       }
      ]
     }
    },
    "State": "ENABLED",
    "Targets": [
     {
      "Arn": {
       "Fn::GetAtt": [
        "EKSGITIAMReportBuildFunction6A136575",
        "Arn"
       ]
      },
      "Id": "Target0"
     }
    ]
   }
  },
  "EKSGITIAMBuildCompleteRuleAllowEventRulefleetworkshopEKSGITIAMReportBuildFunction507D56A92BE75D71": {
   "Type": "AWS::Lambda::Permission",
   "Properties": {
    "Action": "lambda:InvokeFunction",
    "FunctionName": {
     "Fn::GetAtt": [
      "EKSGITIAMReportBuildFunction6A136575",
      "Arn"
     ]
    },
    "Principal": "events.amazonaws.com",
    "SourceArn": {
     "Fn::GetAtt": [
      "EKSGITIAMBuildCompleteRule67653565",
      "Arn"
     ]
    }
   }
  },
  "EKSGITIAMClusterStackB3C8C6A1": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "EKSGITIAMStartBuildFunction2A4FA591",
      "Arn"
     ]
    },
    "ProjectName": {
     "Ref": "EKSGITIAMStackDeployProjectE1461187"
    },
    "CodeBuildIamRoleArn": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "ContentHash": "9ae40ceeec07a042d12397dc0140f44a"
   },
   "DependsOn": [
    "EKSGITIAMBuildCompleteRuleAllowEventRulefleetworkshopEKSGITIAMReportBuildFunction507D56A92BE75D71",
    "EKSGITIAMBuildCompleteRule67653565",
    "EKSGITIAMReportBuildFunction6A136575",
    "EKSGITIAMReportBuildFunctionServiceRoleDefaultPolicy60A9BBDC",
    "EKSGITIAMReportBuildFunctionServiceRole2D8C0032",
    "IDEFleetIdeBootstrapResource98864E55",
    "IDEFleetIdeBootstrapWaitConditionBAEC1144"
   ],
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "EKSHubStackDeployProject89DD576F": {
   "Type": "AWS::CodeBuild::Project",
   "Properties": {
    "Artifacts": {
     "Type": "NO_ARTIFACTS"
    },
    "Cache": {
     "Type": "NO_CACHE"
    },
    "EncryptionKey": "alias/aws/s3",
    "Environment": {
     "ComputeType": "BUILD_GENERAL1_SMALL",
     "EnvironmentVariables": [
      {
       "Name": "TFSTATE_BUCKET_NAME",
       "Type": "PLAINTEXT",
       "Value": {
        "Ref": "TFStateBackendBucketF0FC9A9D"
       }
      },
      {
       "Name": "WORKSHOP_GIT_URL",
       "Type": "PLAINTEXT",
       "Value": "https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop"
      },
      {
       "Name": "WORKSHOP_GIT_BRANCH",
       "Type": "PLAINTEXT",
       "Value": "v1.0.2"
      },
      {
       "Name": "FORCE_DELETE_VPC",
       "Type": "PLAINTEXT",
       "Value": "true"
      }
     ],
     "Image": "aws/codebuild/amazonlinux2-x86_64-standard:4.0",
     "ImagePullCredentialsType": "CODEBUILD",
     "PrivilegedMode": false,
     "Type": "LINUX_CONTAINER"
    },
    "ServiceRole": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "Source": {
     "BuildSpec": "version: 0.2\nphases:\n  pre_build:\n    commands:\n      - |\n        yum install -y gettext\n        # Helm installation\n        curl --silent --location \"https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz\" | tar xz -C /tmp\n        mv /tmp/linux-amd64/helm /usr/local/bin\n        chmod +x /usr/local/bin/helm\n        # Terraform installation\n        sudo yum install -y yum-utils\n        sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\n        sudo yum -y install terraform\n  build:\n    commands:\n      - |\n        set -x\n        set -e\n\n        aws configure set cli_pager \"\"\n\n        ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n        BUCKET_NAME=${TFSTATE_BUCKET_NAME}\n        BASE_DIR=${CODEBUILD_SRC_DIR}\n        WORKSHOP_GIT_URL=${WORKSHOP_GIT_URL:-https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop}\n        WORKSHOP_GIT_BRANCH=${WORKSHOP_GIT_BRANCH:-v1.0.2}\n\n        env | sort\n\n        # clone and create CodeCommit repo\n        git clone $WORKSHOP_GIT_URL $BASE_DIR || true\n        cd $BASE_DIR\n        git checkout $WORKSHOP_GIT_BRANCH\n        cd -\n\n        aws ssm put-parameter --name \"tf-backend-bucket\" --type \"String\" --value \"$BUCKET_NAME\" --overwrite\n\n        # Hub backend config\n        cat << EOT > $BASE_DIR/terraform/hub/backend_override.tf\n        terraform {\n          backend \"s3\" {\n            bucket         = \"$BUCKET_NAME\"\n            key            = \"hub/terraform.tfstate\"\n            region         = \"$AWS_REGION\"\n          }\n        }\n        EOT\n\n        # Fix terraform provider version conflicts\n        echo \"Fixing terraform provider version conflicts...\"\n        rm -f $BASE_DIR/terraform/hub/.terraform.lock.hcl\n        rm -rf $BASE_DIR/terraform/hub/.terraform\n        \n        # Force AWS provider to use compatible version\n        cat > $BASE_DIR/terraform/hub/provider_override.tf << 'EOF'\n        terraform {\n          required_providers {\n            aws = {\n              source  = \"hashicorp/aws\"\n              version = \"= 5.95.0\"\n            }\n          }\n        }\n        EOF\n        \n\n        \n        # Fix terraform init --upgrade in scripts to prevent provider version conflicts\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/hub/deploy.sh\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/hub/destroy.sh\n\n        mkdir -p ~/.ssh\n\n        if [[ $REQUESTED_ACTION == 'Delete' ]]; then\n          # Cleanup scripts\n          # Initialize terraform first since we cleaned up lock files\n          cd $BASE_DIR/terraform/hub\n          terraform init\n          cd -\n          DEBUG=1 $BASE_DIR/terraform/hub/destroy.sh\n\n        else\n          # Project Creation\n          DEBUG=1 $BASE_DIR/terraform/hub/deploy.sh\n        fi\n  post_build:\n    commands:\n      - echo \">>> build status $CODEBUILD_BUILD_SUCCEEDING \"\n",
     "Type": "NO_SOURCE"
    },
    "TimeoutInMinutes": 90
   }
  },
  "EKSHubStartBuildFunctionServiceRole870A7803": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSHubStartBuildFunctionServiceRoleDefaultPolicy1C8EB47F": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "codebuild:StartBuild",
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSHubStackDeployProject89DD576F",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSHubStartBuildFunctionServiceRoleDefaultPolicy1C8EB47F",
    "Roles": [
     {
      "Ref": "EKSHubStartBuildFunctionServiceRole870A7803"
     }
    ]
   }
  },
  "EKSHubStartBuildFunction3A1CEFE8": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n  try {\n    const projectName = event.ResourceProperties.ProjectName;\n    const codeBuildIamRoleArn = event.ResourceProperties.CodeBuildIamRoleArn\n\n    const codebuild = new AWS.CodeBuild();\n\n    console.log(`Starting new build of project ${projectName}`);\n\n    const { build } = await codebuild.startBuild({\n      projectName,\n      // Pass CFN related parameters through the build for extraction by the\n      // completion handler.\n      environmentVariablesOverride: [\n        {\n          name: 'CFN_RESPONSE_URL',\n          value: event.ResponseURL\n        },\n        {\n          name: 'CFN_STACK_ID',\n          value: event.StackId\n        },\n        {\n          name: 'CFN_REQUEST_ID',\n          value: event.RequestId\n        },\n        {\n          name: 'CFN_LOGICAL_RESOURCE_ID',\n          value: event.LogicalResourceId\n        },\n        {\n          name: 'REQUESTED_ACTION',\n          value: event.RequestType\n        },\n        {\n          name: 'RESOURCE_CODEBUILD_ROLE_ARN',\n          value: codeBuildIamRoleArn\n        }\n      ]\n    }).promise();\n    console.log(`Build id ${build.id} started - resource completion handled by EventBridge`);\n  } catch(error) {\n    console.error(error);\n    await respond(event, context, 'FAILED', { Error: error });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSHubStartBuildFunctionServiceRole870A7803",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSHubStartBuildFunctionServiceRoleDefaultPolicy1C8EB47F",
    "EKSHubStartBuildFunctionServiceRole870A7803"
   ]
  },
  "EKSHubReportBuildFunctionServiceRole75FC8EF3": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSHubReportBuildFunctionServiceRoleDefaultPolicyF365714B": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "codebuild:BatchGetBuilds",
        "codebuild:ListBuildsForProject"
       ],
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSHubStackDeployProject89DD576F",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSHubReportBuildFunctionServiceRoleDefaultPolicyF365714B",
    "Roles": [
     {
      "Ref": "EKSHubReportBuildFunctionServiceRole75FC8EF3"
     }
    ]
   }
  },
  "EKSHubReportBuildFunctionDC253ED5": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n\n  const projectName = event['detail']['project-name'];\n\n  const codebuild = new AWS.CodeBuild();\n\n  const buildId = event['detail']['build-id'];\n  const { builds } = await codebuild.batchGetBuilds({\n    ids: [ buildId ]\n  }).promise();\n\n  console.log(JSON.stringify(builds, null, 4));\n\n  const build = builds[0];\n  // Fetch the CFN resource and response parameters from the build environment.\n  const environment = {};\n  build.environment.environmentVariables.forEach(e => environment[e.name] = e.value);\n\n  const response = {\n    ResponseURL: environment.CFN_RESPONSE_URL,\n    StackId: environment.CFN_STACK_ID,\n    LogicalResourceId: environment.CFN_LOGICAL_RESOURCE_ID,\n    RequestId: environment.CFN_REQUEST_ID\n  };\n\n  if (event['detail']['build-status'] === 'SUCCEEDED') {\n    await respond(response, context, 'SUCCESS', {}, 'build');\n  } else {\n    await respond(response, context, 'FAILED', { Error: 'Build failed' });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSHubReportBuildFunctionServiceRole75FC8EF3",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSHubReportBuildFunctionServiceRoleDefaultPolicyF365714B",
    "EKSHubReportBuildFunctionServiceRole75FC8EF3"
   ]
  },
  "EKSHubBuildCompleteRule63C7CF95": {
   "Type": "AWS::Events::Rule",
   "Properties": {
    "Description": "Build complete",
    "EventPattern": {
     "source": [
      "aws.codebuild"
     ],
     "detail-type": [
      "CodeBuild Build State Change"
     ],
     "detail": {
      "build-status": [
       "SUCCEEDED",
       "FAILED",
       "STOPPED"
      ],
      "project-name": [
       {
        "Ref": "EKSHubStackDeployProject89DD576F"
       }
      ]
     }
    },
    "State": "ENABLED",
    "Targets": [
     {
      "Arn": {
       "Fn::GetAtt": [
        "EKSHubReportBuildFunctionDC253ED5",
        "Arn"
       ]
      },
      "Id": "Target0"
     }
    ]
   }
  },
  "EKSHubBuildCompleteRuleAllowEventRulefleetworkshopEKSHubReportBuildFunction40DCCA2EA68762D8": {
   "Type": "AWS::Lambda::Permission",
   "Properties": {
    "Action": "lambda:InvokeFunction",
    "FunctionName": {
     "Fn::GetAtt": [
      "EKSHubReportBuildFunctionDC253ED5",
      "Arn"
     ]
    },
    "Principal": "events.amazonaws.com",
    "SourceArn": {
     "Fn::GetAtt": [
      "EKSHubBuildCompleteRule63C7CF95",
      "Arn"
     ]
    }
   }
  },
  "EKSHubClusterStack794113DD": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "EKSHubStartBuildFunction3A1CEFE8",
      "Arn"
     ]
    },
    "ProjectName": {
     "Ref": "EKSHubStackDeployProject89DD576F"
    },
    "CodeBuildIamRoleArn": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "ContentHash": "86237cabaaa4925bf6df8813d6aa4cae"
   },
   "DependsOn": [
    "EKSGITIAMClusterStackB3C8C6A1",
    "EKSHubBuildCompleteRuleAllowEventRulefleetworkshopEKSHubReportBuildFunction40DCCA2EA68762D8",
    "EKSHubBuildCompleteRule63C7CF95",
    "EKSHubReportBuildFunctionDC253ED5",
    "EKSHubReportBuildFunctionServiceRoleDefaultPolicyF365714B",
    "EKSHubReportBuildFunctionServiceRole75FC8EF3"
   ],
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "participantAccessEntryEksClusterHub9258E486": {
   "Type": "AWS::EKS::AccessEntry",
   "Condition": "HasParticipantRole",
   "Properties": {
    "AccessPolicies": [
     {
      "AccessScope": {
       "Type": "cluster"
      },
      "PolicyArn": {
       "Fn::Join": [
        "",
        [
         "arn:",
         {
          "Ref": "AWS::Partition"
         },
         ":eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
        ]
       ]
      }
     }
    ],
    "ClusterName": "fleet-hub-cluster",
    "PrincipalArn": {
     "Ref": "ParticipantAssumedRoleArn"
    }
   },
   "DependsOn": [
    "EKSHubClusterStack794113DD"
   ]
  },
  "EKSSpokestagingStackDeployProjectE3D9EDE7": {
   "Type": "AWS::CodeBuild::Project",
   "Properties": {
    "Artifacts": {
     "Type": "NO_ARTIFACTS"
    },
    "Cache": {
     "Type": "NO_CACHE"
    },
    "EncryptionKey": "alias/aws/s3",
    "Environment": {
     "ComputeType": "BUILD_GENERAL1_SMALL",
     "EnvironmentVariables": [
      {
       "Name": "TFSTATE_BUCKET_NAME",
       "Type": "PLAINTEXT",
       "Value": {
        "Ref": "TFStateBackendBucketF0FC9A9D"
       }
      },
      {
       "Name": "WORKSHOP_GIT_URL",
       "Type": "PLAINTEXT",
       "Value": "https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop"
      },
      {
       "Name": "WORKSHOP_GIT_BRANCH",
       "Type": "PLAINTEXT",
       "Value": "v1.0.2"
      },
      {
       "Name": "FORCE_DELETE_VPC",
       "Type": "PLAINTEXT",
       "Value": "true"
      },
      {
       "Name": "SPOKE",
       "Type": "PLAINTEXT", 
       "Value": "staging"
      }
     ],
     "Image": "aws/codebuild/amazonlinux2-x86_64-standard:4.0",
     "ImagePullCredentialsType": "CODEBUILD",
     "PrivilegedMode": false,
     "Type": "LINUX_CONTAINER"
    },
    "ServiceRole": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "Source": {
     "BuildSpec": "version: 0.2\nphases:\n  pre_build:\n    commands:\n      - |\n        yum install -y gettext\n        # Helm installation\n        curl --silent --location \"https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz\" | tar xz -C /tmp\n        mv /tmp/linux-amd64/helm /usr/local/bin\n        chmod +x /usr/local/bin/helm\n        # Terraform installation\n        sudo yum install -y yum-utils\n        sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\n        sudo yum -y install terraform\n  build:\n    commands:\n      - |\n        set -x\n        set -e\n\n        aws configure set cli_pager \"\"\n\n        ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n        BUCKET_NAME=${TFSTATE_BUCKET_NAME}\n        BASE_DIR=${CODEBUILD_SRC_DIR}\n        WORKSHOP_GIT_URL=${WORKSHOP_GIT_URL:-https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop}\n        WORKSHOP_GIT_BRANCH=${WORKSHOP_GIT_BRANCH:-v1.0.2}\n\n        env | sort\n\n        # clone and create CodeCommit repo\n        git clone $WORKSHOP_GIT_URL $BASE_DIR || true\n        cd $BASE_DIR\n        git checkout $WORKSHOP_GIT_BRANCH\n        cd -\n\n        # This gives access to the EKS cluster in terraform\n\n        # Spokes backend config\n        cat << EOT > $BASE_DIR/terraform/spokes/backend_override.tf\n        terraform {\n          backend \"s3\" {\n            bucket         = \"$BUCKET_NAME\"\n            key            = \"spokes/terraform.tfstate\"\n            region         = \"$AWS_REGION\"\n            workspace_key_prefix = \"spokes\"\n          }\n        }\n        EOT\n\n        # Fix terraform provider version conflicts\n        echo \"Fixing terraform provider version conflicts...\"\n        rm -f $BASE_DIR/terraform/spokes/.terraform.lock.hcl\n        rm -rf $BASE_DIR/terraform/spokes/.terraform\n        \n        # Clean up any existing terraform state to prevent dependency issues\n        rm -f $BASE_DIR/terraform/spokes/terraform.tfstate*\n        \n        # Clean up workspace state directory to prevent workspace conflicts\n        rm -rf $BASE_DIR/terraform/spokes/terraform.tfstate.d\n\n        # Force AWS provider to use compatible version (overrides all other constraints)\n        cat > $BASE_DIR/terraform/spokes/provider_override.tf << 'EOF'\n        terraform {\n          required_providers {\n            aws = {\n              source  = \"hashicorp/aws\"\n              version = \"= 5.95.0\"\n            }\n          }\n        }\n        EOF\n\n        # Fix terraform init --upgrade in scripts to prevent provider version conflicts\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/spokes/deploy.sh\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/spokes/destroy.sh\n        \n        # Fix workspace selection issue in deploy script\n        sed -i 's/terraform -chdir=\\$SCRIPTDIR workspace select -or-create \\$env/terraform -chdir=\\$SCRIPTDIR workspace select \\$env 2>\\/dev\\/null || terraform -chdir=\\$SCRIPTDIR workspace new \\$env/' $BASE_DIR/terraform/spokes/deploy.sh\n\n        mkdir -p ~/.ssh\n\n        # Add better error handling and debugging\n        set +e  # Temporarily disable exit on error for better debugging\n        \n        if [[ $REQUESTED_ACTION == 'Delete' ]]; then\n          echo \"=== STARTING DESTROY PROCESS FOR ${SPOKE} ===\"\n          cd $BASE_DIR/terraform/spokes\n          echo \"Current directory: $(pwd)\"\n          echo \"Files in directory: $(ls -la)\"\n          \n          echo \"Running terraform init...\"\n          terraform init\n          INIT_EXIT_CODE=$?\n          if [ $INIT_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: terraform init failed with exit code $INIT_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Setting up workspace ${SPOKE}...\"\n          terraform workspace select ${SPOKE} 2>/dev/null || terraform workspace new ${SPOKE}\n          WORKSPACE_EXIT_CODE=$?\n          if [ $WORKSPACE_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: workspace setup failed with exit code $WORKSPACE_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Current terraform workspace: $(terraform workspace show)\"\n          cd -\n          \n          echo \"Running destroy script...\"\n          DEBUG=1 $BASE_DIR/terraform/spokes/destroy.sh ${SPOKE}\n          DESTROY_EXIT_CODE=$?\n          if [ $DESTROY_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: destroy script failed with exit code $DESTROY_EXIT_CODE\"\n            exit 1\n          fi\n        else\n          echo \"=== STARTING DEPLOYMENT PROCESS FOR ${SPOKE} ===\"\n          cd $BASE_DIR/terraform/spokes\n          echo \"Current directory: $(pwd)\"\n          echo \"Files in directory: $(ls -la)\"\n          \n          echo \"Running terraform init...\"\n          terraform init\n          INIT_EXIT_CODE=$?\n          if [ $INIT_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: terraform init failed with exit code $INIT_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Setting up workspace ${SPOKE}...\"\n          terraform workspace select ${SPOKE} 2>/dev/null || terraform workspace new ${SPOKE}\n          WORKSPACE_EXIT_CODE=$?\n          if [ $WORKSPACE_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: workspace setup failed with exit code $WORKSPACE_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Current terraform workspace: $(terraform workspace show)\"\n          cd -\n          \n          echo \"Running deploy script...\"\n          DEBUG=1 $BASE_DIR/terraform/spokes/deploy.sh ${SPOKE}\n          DEPLOY_EXIT_CODE=$?\n          if [ $DEPLOY_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: deploy script failed with exit code $DEPLOY_EXIT_CODE\"\n            exit 1\n          fi\n        fi\n        \n        set -e  # Re-enable exit on error\n        echo \"=== PROCESS COMPLETED SUCCESSFULLY ===\"\n  post_build:\n    commands:\n      - echo \">>> build status $CODEBUILD_BUILD_SUCCEEDING \"\n",
     "Type": "NO_SOURCE"
    },
    "TimeoutInMinutes": 90
   }
  },
  "EKSSpokestagingStartBuildFunctionServiceRole8B06DBA2": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSSpokestagingStartBuildFunctionServiceRoleDefaultPolicy712301C9": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "codebuild:StartBuild",
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSSpokestagingStackDeployProjectE3D9EDE7",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSSpokestagingStartBuildFunctionServiceRoleDefaultPolicy712301C9",
    "Roles": [
     {
      "Ref": "EKSSpokestagingStartBuildFunctionServiceRole8B06DBA2"
     }
    ]
   }
  },
  "EKSSpokestagingStartBuildFunctionCA1B8C5A": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n  try {\n    const projectName = event.ResourceProperties.ProjectName;\n    const codeBuildIamRoleArn = event.ResourceProperties.CodeBuildIamRoleArn\n\n    const codebuild = new AWS.CodeBuild();\n\n    console.log(`Starting new build of project ${projectName}`);\n\n    const { build } = await codebuild.startBuild({\n      projectName,\n      // Pass CFN related parameters through the build for extraction by the\n      // completion handler.\n      environmentVariablesOverride: [\n        {\n          name: 'CFN_RESPONSE_URL',\n          value: event.ResponseURL\n        },\n        {\n          name: 'CFN_STACK_ID',\n          value: event.StackId\n        },\n        {\n          name: 'CFN_REQUEST_ID',\n          value: event.RequestId\n        },\n        {\n          name: 'CFN_LOGICAL_RESOURCE_ID',\n          value: event.LogicalResourceId\n        },\n        {\n          name: 'REQUESTED_ACTION',\n          value: event.RequestType\n        },\n        {\n          name: 'RESOURCE_CODEBUILD_ROLE_ARN',\n          value: codeBuildIamRoleArn\n        }\n      ]\n    }).promise();\n    console.log(`Build id ${build.id} started - resource completion handled by EventBridge`);\n  } catch(error) {\n    console.error(error);\n    await respond(event, context, 'FAILED', { Error: error });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSSpokestagingStartBuildFunctionServiceRole8B06DBA2",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSSpokestagingStartBuildFunctionServiceRoleDefaultPolicy712301C9",
    "EKSSpokestagingStartBuildFunctionServiceRole8B06DBA2"
   ]
  },
  "EKSSpokestagingReportBuildFunctionServiceRoleB1A8B86C": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSSpokestagingReportBuildFunctionServiceRoleDefaultPolicy5407AF58": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "codebuild:BatchGetBuilds",
        "codebuild:ListBuildsForProject"
       ],
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSSpokestagingStackDeployProjectE3D9EDE7",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSSpokestagingReportBuildFunctionServiceRoleDefaultPolicy5407AF58",
    "Roles": [
     {
      "Ref": "EKSSpokestagingReportBuildFunctionServiceRoleB1A8B86C"
     }
    ]
   }
  },
  "EKSSpokestagingReportBuildFunctionF8FEF0A1": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n\n  const projectName = event['detail']['project-name'];\n\n  const codebuild = new AWS.CodeBuild();\n\n  const buildId = event['detail']['build-id'];\n  const { builds } = await codebuild.batchGetBuilds({\n    ids: [ buildId ]\n  }).promise();\n\n  console.log(JSON.stringify(builds, null, 4));\n\n  const build = builds[0];\n  // Fetch the CFN resource and response parameters from the build environment.\n  const environment = {};\n  build.environment.environmentVariables.forEach(e => environment[e.name] = e.value);\n\n  const response = {\n    ResponseURL: environment.CFN_RESPONSE_URL,\n    StackId: environment.CFN_STACK_ID,\n    LogicalResourceId: environment.CFN_LOGICAL_RESOURCE_ID,\n    RequestId: environment.CFN_REQUEST_ID\n  };\n\n  if (event['detail']['build-status'] === 'SUCCEEDED') {\n    await respond(response, context, 'SUCCESS', {}, 'build');\n  } else {\n    await respond(response, context, 'FAILED', { Error: 'Build failed' });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSSpokestagingReportBuildFunctionServiceRoleB1A8B86C",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSSpokestagingReportBuildFunctionServiceRoleDefaultPolicy5407AF58",
    "EKSSpokestagingReportBuildFunctionServiceRoleB1A8B86C"
   ]
  },
  "EKSSpokestagingBuildCompleteRule11DC7A50": {
   "Type": "AWS::Events::Rule",
   "Properties": {
    "Description": "Build complete",
    "EventPattern": {
     "source": [
      "aws.codebuild"
     ],
     "detail-type": [
      "CodeBuild Build State Change"
     ],
     "detail": {
      "build-status": [
       "SUCCEEDED",
       "FAILED",
       "STOPPED"
      ],
      "project-name": [
       {
        "Ref": "EKSSpokestagingStackDeployProjectE3D9EDE7"
       }
      ]
     }
    },
    "State": "ENABLED",
    "Targets": [
     {
      "Arn": {
       "Fn::GetAtt": [
        "EKSSpokestagingReportBuildFunctionF8FEF0A1",
        "Arn"
       ]
      },
      "Id": "Target0"
     }
    ]
   }
  },
  "EKSSpokestagingBuildCompleteRuleAllowEventRulefleetworkshopEKSSpokestagingReportBuildFunction3118703B84B80F40": {
   "Type": "AWS::Lambda::Permission",
   "Properties": {
    "Action": "lambda:InvokeFunction",
    "FunctionName": {
     "Fn::GetAtt": [
      "EKSSpokestagingReportBuildFunctionF8FEF0A1",
      "Arn"
     ]
    },
    "Principal": "events.amazonaws.com",
    "SourceArn": {
     "Fn::GetAtt": [
      "EKSSpokestagingBuildCompleteRule11DC7A50",
      "Arn"
     ]
    }
   }
  },
  "EKSSpokestagingClusterStack5F44D7D8": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "EKSSpokestagingStartBuildFunctionCA1B8C5A",
      "Arn"
     ]
    },
    "ProjectName": {
     "Ref": "EKSSpokestagingStackDeployProjectE3D9EDE7"
    },
    "CodeBuildIamRoleArn": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "ContentHash": "ace61dae2606af9b25aad08be91e0a08"
   },
   "DependsOn": [
    "EKSGITIAMClusterStackB3C8C6A1",
    "EKSSpokestagingBuildCompleteRuleAllowEventRulefleetworkshopEKSSpokestagingReportBuildFunction3118703B84B80F40",
    "EKSSpokestagingBuildCompleteRule11DC7A50",
    "EKSSpokestagingReportBuildFunctionF8FEF0A1",
    "EKSSpokestagingReportBuildFunctionServiceRoleDefaultPolicy5407AF58",
    "EKSSpokestagingReportBuildFunctionServiceRoleB1A8B86C"
   ],
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "participantAccessEntrystagingE5810C19": {
   "Type": "AWS::EKS::AccessEntry",
   "Condition": "HasParticipantRole",
   "Properties": {
    "AccessPolicies": [
     {
      "AccessScope": {
       "Type": "cluster"
      },
      "PolicyArn": {
       "Fn::Join": [
        "",
        [
         "arn:",
         {
          "Ref": "AWS::Partition"
         },
         ":eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
        ]
       ]
      }
     }
    ],
    "ClusterName": "fleet-spoke-staging",
    "PrincipalArn": {
     "Ref": "ParticipantAssumedRoleArn"
    }
   },
   "DependsOn": [
    "EKSSpokestagingClusterStack5F44D7D8"
   ]
  },
  "EKSSpokeprodStackDeployProject510818EF": {
   "Type": "AWS::CodeBuild::Project",
   "Properties": {
    "Artifacts": {
     "Type": "NO_ARTIFACTS"
    },
    "Cache": {
     "Type": "NO_CACHE"
    },
    "EncryptionKey": "alias/aws/s3",
    "Environment": {
     "ComputeType": "BUILD_GENERAL1_SMALL",
     "EnvironmentVariables": [
      {
       "Name": "TFSTATE_BUCKET_NAME",
       "Type": "PLAINTEXT",
       "Value": {
        "Ref": "TFStateBackendBucketF0FC9A9D"
       }
      },
      {
       "Name": "WORKSHOP_GIT_URL",
       "Type": "PLAINTEXT",
       "Value": "https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop"
      },
      {
       "Name": "WORKSHOP_GIT_BRANCH",
       "Type": "PLAINTEXT",
       "Value": "v1.0.2"
      },
      {
       "Name": "FORCE_DELETE_VPC",
       "Type": "PLAINTEXT",
       "Value": "true"
      },
      {
       "Name": "SPOKE",
       "Type": "PLAINTEXT",
       "Value": "prod"
      }
     ],
     "Image": "aws/codebuild/amazonlinux2-x86_64-standard:4.0",
     "ImagePullCredentialsType": "CODEBUILD",
     "PrivilegedMode": false,
     "Type": "LINUX_CONTAINER"
    },
    "ServiceRole": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "Source": {
     "BuildSpec": "version: 0.2\nphases:\n  pre_build:\n    commands:\n      - |\n        yum install -y gettext\n        # Helm installation\n        curl --silent --location \"https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz\" | tar xz -C /tmp\n        mv /tmp/linux-amd64/helm /usr/local/bin\n        chmod +x /usr/local/bin/helm\n        # Terraform installation\n        sudo yum install -y yum-utils\n        sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\n        sudo yum -y install terraform\n  build:\n    commands:\n      - |\n        set -x\n        set -e\n\n        aws configure set cli_pager \"\"\n\n        ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n        BUCKET_NAME=${TFSTATE_BUCKET_NAME}\n        BASE_DIR=${CODEBUILD_SRC_DIR}\n        WORKSHOP_GIT_URL=${WORKSHOP_GIT_URL:-https://github.com/aws-samples/fleet-management-on-amazon-eks-workshop}\n        WORKSHOP_GIT_BRANCH=${WORKSHOP_GIT_BRANCH:-v1.0.2}\n\n        env | sort\n\n        # clone and create CodeCommit repo\n        git clone $WORKSHOP_GIT_URL $BASE_DIR || true\n        cd $BASE_DIR\n        git checkout $WORKSHOP_GIT_BRANCH\n        cd -\n\n        # This gives access to the EKS cluster in terraform\n\n        # Spokes backend config\n        cat << EOT > $BASE_DIR/terraform/spokes/backend_override.tf\n        terraform {\n          backend \"s3\" {\n            bucket         = \"$BUCKET_NAME\"\n            key            = \"spokes/terraform.tfstate\"\n            region         = \"$AWS_REGION\"\n            workspace_key_prefix = \"spokes\"\n          }\n        }\n        EOT\n\n        # Fix terraform provider version conflicts\n        echo \"Fixing terraform provider version conflicts...\"\n        rm -f $BASE_DIR/terraform/spokes/.terraform.lock.hcl\n        rm -rf $BASE_DIR/terraform/spokes/.terraform\n        \n        # Clean up any existing terraform state to prevent dependency issues\n        rm -f $BASE_DIR/terraform/spokes/terraform.tfstate*\n        \n        # Clean up workspace state directory to prevent workspace conflicts\n        rm -rf $BASE_DIR/terraform/spokes/terraform.tfstate.d\n\n        # Force AWS provider to use compatible version (overrides all other constraints)\n        cat > $BASE_DIR/terraform/spokes/provider_override.tf << 'EOF'\n        terraform {\n          required_providers {\n            aws = {\n              source  = \"hashicorp/aws\"\n              version = \"= 5.95.0\"\n            }\n          }\n        }\n        EOF\n\n        # Fix terraform init --upgrade in scripts to prevent provider version conflicts\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/spokes/deploy.sh\n        sed -i 's/ --upgrade//' $BASE_DIR/terraform/spokes/destroy.sh\n        \n        # Fix workspace selection issue in deploy script\n        sed -i 's/terraform -chdir=\\$SCRIPTDIR workspace select -or-create \\$env/terraform -chdir=\\$SCRIPTDIR workspace select \\$env 2>\\/dev\\/null || terraform -chdir=\\$SCRIPTDIR workspace new \\$env/' $BASE_DIR/terraform/spokes/deploy.sh\n\n        mkdir -p ~/.ssh\n\n        # Add better error handling and debugging\n        set +e  # Temporarily disable exit on error for better debugging\n        \n        if [[ $REQUESTED_ACTION == 'Delete' ]]; then\n          echo \"=== STARTING DESTROY PROCESS FOR ${SPOKE} ===\"\n          cd $BASE_DIR/terraform/spokes\n          echo \"Current directory: $(pwd)\"\n          echo \"Files in directory: $(ls -la)\"\n          \n          echo \"Running terraform init...\"\n          terraform init\n          INIT_EXIT_CODE=$?\n          if [ $INIT_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: terraform init failed with exit code $INIT_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Setting up workspace ${SPOKE}...\"\n          terraform workspace select ${SPOKE} 2>/dev/null || terraform workspace new ${SPOKE}\n          WORKSPACE_EXIT_CODE=$?\n          if [ $WORKSPACE_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: workspace setup failed with exit code $WORKSPACE_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Current terraform workspace: $(terraform workspace show)\"\n          cd -\n          \n          echo \"Running destroy script...\"\n          DEBUG=1 $BASE_DIR/terraform/spokes/destroy.sh ${SPOKE}\n          DESTROY_EXIT_CODE=$?\n          if [ $DESTROY_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: destroy script failed with exit code $DESTROY_EXIT_CODE\"\n            exit 1\n          fi\n        else\n          echo \"=== STARTING DEPLOYMENT PROCESS FOR ${SPOKE} ===\"\n          cd $BASE_DIR/terraform/spokes\n          echo \"Current directory: $(pwd)\"\n          echo \"Files in directory: $(ls -la)\"\n          \n          echo \"Running terraform init...\"\n          terraform init\n          INIT_EXIT_CODE=$?\n          if [ $INIT_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: terraform init failed with exit code $INIT_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Setting up workspace ${SPOKE}...\"\n          terraform workspace select ${SPOKE} 2>/dev/null || terraform workspace new ${SPOKE}\n          WORKSPACE_EXIT_CODE=$?\n          if [ $WORKSPACE_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: workspace setup failed with exit code $WORKSPACE_EXIT_CODE\"\n            exit 1\n          fi\n          \n          echo \"Current terraform workspace: $(terraform workspace show)\"\n          cd -\n          \n          echo \"Running deploy script...\"\n          DEBUG=1 $BASE_DIR/terraform/spokes/deploy.sh ${SPOKE}\n          DEPLOY_EXIT_CODE=$?\n          if [ $DEPLOY_EXIT_CODE -ne 0 ]; then\n            echo \"ERROR: deploy script failed with exit code $DEPLOY_EXIT_CODE\"\n            exit 1\n          fi\n        fi\n        \n        set -e  # Re-enable exit on error\n        echo \"=== PROCESS COMPLETED SUCCESSFULLY ===\"\n  post_build:\n    commands:\n      - echo \">>> build status $CODEBUILD_BUILD_SUCCEEDING \"\n",
     "Type": "NO_SOURCE"
    },
    "TimeoutInMinutes": 90
   }
  },
  "EKSSpokeprodStartBuildFunctionServiceRole1E61918E": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSSpokeprodStartBuildFunctionServiceRoleDefaultPolicyB6A7221A": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": "codebuild:StartBuild",
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSSpokeprodStackDeployProject510818EF",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSSpokeprodStartBuildFunctionServiceRoleDefaultPolicyB6A7221A",
    "Roles": [
     {
      "Ref": "EKSSpokeprodStartBuildFunctionServiceRole1E61918E"
     }
    ]
   }
  },
  "EKSSpokeprodStartBuildFunctionA4E4EBC9": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n  try {\n    const projectName = event.ResourceProperties.ProjectName;\n    const codeBuildIamRoleArn = event.ResourceProperties.CodeBuildIamRoleArn\n\n    const codebuild = new AWS.CodeBuild();\n\n    console.log(`Starting new build of project ${projectName}`);\n\n    const { build } = await codebuild.startBuild({\n      projectName,\n      // Pass CFN related parameters through the build for extraction by the\n      // completion handler.\n      environmentVariablesOverride: [\n        {\n          name: 'CFN_RESPONSE_URL',\n          value: event.ResponseURL\n        },\n        {\n          name: 'CFN_STACK_ID',\n          value: event.StackId\n        },\n        {\n          name: 'CFN_REQUEST_ID',\n          value: event.RequestId\n        },\n        {\n          name: 'CFN_LOGICAL_RESOURCE_ID',\n          value: event.LogicalResourceId\n        },\n        {\n          name: 'REQUESTED_ACTION',\n          value: event.RequestType\n        },\n        {\n          name: 'RESOURCE_CODEBUILD_ROLE_ARN',\n          value: codeBuildIamRoleArn\n        }\n      ]\n    }).promise();\n    console.log(`Build id ${build.id} started - resource completion handled by EventBridge`);\n  } catch(error) {\n    console.error(error);\n    await respond(event, context, 'FAILED', { Error: error });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSSpokeprodStartBuildFunctionServiceRole1E61918E",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSSpokeprodStartBuildFunctionServiceRoleDefaultPolicyB6A7221A",
    "EKSSpokeprodStartBuildFunctionServiceRole1E61918E"
   ]
  },
  "EKSSpokeprodReportBuildFunctionServiceRole513EB37D": {
   "Type": "AWS::IAM::Role",
   "Properties": {
    "AssumeRolePolicyDocument": {
     "Statement": [
      {
       "Action": "sts:AssumeRole",
       "Effect": "Allow",
       "Principal": {
        "Service": "lambda.amazonaws.com"
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "ManagedPolicyArns": [
     {
      "Fn::Join": [
       "",
       [
        "arn:",
        {
         "Ref": "AWS::Partition"
        },
        ":iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
       ]
      ]
     }
    ]
   }
  },
  "EKSSpokeprodReportBuildFunctionServiceRoleDefaultPolicy7EF33DE3": {
   "Type": "AWS::IAM::Policy",
   "Properties": {
    "PolicyDocument": {
     "Statement": [
      {
       "Action": [
        "codebuild:BatchGetBuilds",
        "codebuild:ListBuildsForProject"
       ],
       "Effect": "Allow",
       "Resource": {
        "Fn::GetAtt": [
         "EKSSpokeprodStackDeployProject510818EF",
         "Arn"
        ]
       }
      }
     ],
     "Version": "2012-10-17"
    },
    "PolicyName": "EKSSpokeprodReportBuildFunctionServiceRoleDefaultPolicy7EF33DE3",
    "Roles": [
     {
      "Ref": "EKSSpokeprodReportBuildFunctionServiceRole513EB37D"
     }
    ]
   }
  },
  "EKSSpokeprodReportBuildFunction08CDF332": {
   "Type": "AWS::Lambda::Function",
   "Properties": {
    "Code": {
     "ZipFile": "// This function is based on the cfnresponse JS module that is published\n// by CloudFormation. It's an async function that makes coding much easier.\nconst respond = async function(event, context, responseStatus, responseData, physicalResourceId, noEcho) {\n  return new Promise((resolve, reject) => {\n    var responseBody = JSON.stringify({\n        Status: responseStatus,\n        Reason: \"See the details in CloudWatch Log Stream: \" + context.logGroupName + \" \" + context.logStreamName,\n        PhysicalResourceId: physicalResourceId || context.logStreamName,\n        StackId: event.StackId,\n        RequestId: event.RequestId,\n        LogicalResourceId: event.LogicalResourceId,\n        NoEcho: noEcho || false,\n        Data: responseData\n    });\n\n    console.log(\"Response body:\\\\n\", responseBody);\n\n    var https = require(\"https\");\n    var url = require(\"url\");\n\n    var parsedUrl = url.parse(event.ResponseURL);\n    var options = {\n        hostname: parsedUrl.hostname,\n        port: 443,\n        path: parsedUrl.path,\n        method: \"PUT\",\n        headers: {\n            \"content-type\": \"\",\n            \"content-length\": responseBody.length\n        }\n    };\n\n    var request = https.request(options, function(response) {\n        console.log(\"Status code: \" + response.statusCode);\n        console.log(\"Status message: \" + response.statusMessage);\n        resolve();\n    });\n\n    request.on(\"error\", function(error) {\n        console.log(\"respond(..) failed executing https.request(..): \" + error);\n        resolve();\n    });\n\n    request.write(responseBody);\n    request.end();\n  });\n};\nconst AWS = require('aws-sdk');\n\nexports.handler = async function (event, context) {\n  console.log(JSON.stringify(event, null, 4));\n\n  const projectName = event['detail']['project-name'];\n\n  const codebuild = new AWS.CodeBuild();\n\n  const buildId = event['detail']['build-id'];\n  const { builds } = await codebuild.batchGetBuilds({\n    ids: [ buildId ]\n  }).promise();\n\n  console.log(JSON.stringify(builds, null, 4));\n\n  const build = builds[0];\n  // Fetch the CFN resource and response parameters from the build environment.\n  const environment = {};\n  build.environment.environmentVariables.forEach(e => environment[e.name] = e.value);\n\n  const response = {\n    ResponseURL: environment.CFN_RESPONSE_URL,\n    StackId: environment.CFN_STACK_ID,\n    LogicalResourceId: environment.CFN_LOGICAL_RESOURCE_ID,\n    RequestId: environment.CFN_REQUEST_ID\n  };\n\n  if (event['detail']['build-status'] === 'SUCCEEDED') {\n    await respond(response, context, 'SUCCESS', {}, 'build');\n  } else {\n    await respond(response, context, 'FAILED', { Error: 'Build failed' });\n  }\n};\n"
    },
    "Handler": "index.handler",
    "Role": {
     "Fn::GetAtt": [
      "EKSSpokeprodReportBuildFunctionServiceRole513EB37D",
      "Arn"
     ]
    },
    "Runtime": "nodejs16.x",
    "Timeout": 60
   },
   "DependsOn": [
    "EKSSpokeprodReportBuildFunctionServiceRoleDefaultPolicy7EF33DE3",
    "EKSSpokeprodReportBuildFunctionServiceRole513EB37D"
   ]
  },
  "EKSSpokeprodBuildCompleteRule125D9181": {
   "Type": "AWS::Events::Rule",
   "Properties": {
    "Description": "Build complete",
    "EventPattern": {
     "source": [
      "aws.codebuild"
     ],
     "detail-type": [
      "CodeBuild Build State Change"
     ],
     "detail": {
      "build-status": [
       "SUCCEEDED",
       "FAILED",
       "STOPPED"
      ],
      "project-name": [
       {
        "Ref": "EKSSpokeprodStackDeployProject510818EF"
       }
      ]
     }
    },
    "State": "ENABLED",
    "Targets": [
     {
      "Arn": {
       "Fn::GetAtt": [
        "EKSSpokeprodReportBuildFunction08CDF332",
        "Arn"
       ]
      },
      "Id": "Target0"
     }
    ]
   }
  },
  "EKSSpokeprodBuildCompleteRuleAllowEventRulefleetworkshopEKSSpokeprodReportBuildFunction9223349AB90CE2DC": {
   "Type": "AWS::Lambda::Permission",
   "Properties": {
    "Action": "lambda:InvokeFunction",
    "FunctionName": {
     "Fn::GetAtt": [
      "EKSSpokeprodReportBuildFunction08CDF332",
      "Arn"
     ]
    },
    "Principal": "events.amazonaws.com",
    "SourceArn": {
     "Fn::GetAtt": [
      "EKSSpokeprodBuildCompleteRule125D9181",
      "Arn"
     ]
    }
   }
  },
  "EKSSpokeprodClusterStackFCCABF4A": {
   "Type": "AWS::CloudFormation::CustomResource",
   "Properties": {
    "ServiceToken": {
     "Fn::GetAtt": [
      "EKSSpokeprodStartBuildFunctionA4E4EBC9",
      "Arn"
     ]
    },
    "ProjectName": {
     "Ref": "EKSSpokeprodStackDeployProject510818EF"
    },
    "CodeBuildIamRoleArn": {
     "Fn::GetAtt": [
      "SharedRoleD1D02F7E",
      "Arn"
     ]
    },
    "ContentHash": "ace61dae2606af9b25aad08be91e0a08"
   },
   "DependsOn": [
    "EKSGITIAMClusterStackB3C8C6A1",
    "EKSSpokeprodBuildCompleteRuleAllowEventRulefleetworkshopEKSSpokeprodReportBuildFunction9223349AB90CE2DC",
    "EKSSpokeprodBuildCompleteRule125D9181",
    "EKSSpokeprodReportBuildFunction08CDF332",
    "EKSSpokeprodReportBuildFunctionServiceRoleDefaultPolicy7EF33DE3",
    "EKSSpokeprodReportBuildFunctionServiceRole513EB37D"
   ],
   "UpdateReplacePolicy": "Delete",
   "DeletionPolicy": "Delete"
  },
  "participantAccessEntryprodE9CA997B": {
   "Type": "AWS::EKS::AccessEntry",
   "Condition": "HasParticipantRole",
   "Properties": {
    "AccessPolicies": [
     {
      "AccessScope": {
       "Type": "cluster"
      },
      "PolicyArn": {
       "Fn::Join": [
        "",
        [
         "arn:",
         {
          "Ref": "AWS::Partition"
         },
         ":eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
        ]
       ]
      }
     }
    ],
    "ClusterName": "fleet-spoke-prod",
    "PrincipalArn": {
     "Ref": "ParticipantAssumedRoleArn"
    }
   },
   "DependsOn": [
    "EKSSpokeprodClusterStackFCCABF4A"
   ]
  },
  "SetupGit": {
   "Type": "AWS::SSM::Document",
   "Properties": {
    "Content": {
     "schemaVersion": "2.2",
     "description": "Setup Git",
     "parameters": {},
     "mainSteps": [
      {
       "action": "aws:runShellScript",
       "name": "SetupGit",
       "inputs": {
        "runCommand": [
         "sudo su - ec2-user -c 'GITOPS_DIR=/home/ec2-user/environment/gitops-repos ~/environment/fleet-management-on-amazon-eks-workshop/setup-git.sh'"
        ]
       }
      }
     ]
    },
    "DocumentFormat": "YAML",
    "DocumentType": "Command",
    "TargetType": "/AWS::EC2::Instance",
    "UpdateMethod": "NewVersion"
   },
   "DependsOn": [
    "EKSHubClusterStack794113DD"
   ]
  },
  "SetupGitAssociation": {
   "Type": "AWS::SSM::Association",
   "Properties": {
    "AssociationName": "SetupGitAssociation",
    "Name": {
     "Ref": "SetupGit"
    },
    "Targets": [
     {
      "Key": "tag:aws:cloudformation:stack-name",
      "Values": [
       "fleet-workshop",
       "fleet-workshop-team-stack"
      ]
     }
    ]
   }
  }
 },
 "Parameters": {
  "SsmParameterValueawsserviceamiamazonlinuxlatestal2023amikernel61x8664C96584B6F00A464EAD1953AFF4B05118Parameter": {
   "Type": "AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>",
   "Default": "/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64"
  },
  "ParticipantAssumedRoleArn": {
   "Type": "String",
   "Default": "",
   "Description": "Optional: ARN of the assumed role for workshop participants to access EKS clusters. Leave empty if not needed."
  }
 },
 "Conditions": {
  "HasParticipantRole": {
   "Fn::Not": [
    {
     "Fn::Equals": [
      {
       "Ref": "ParticipantAssumedRoleArn"
      },
      ""
     ]
    }
   ]
  }
 },
 "Outputs": {
  "IdeUrl": {
   "Value": {
    "Fn::Join": [
     "",
     [
      "https://",
      {
       "Fn::GetAtt": [
        "IDEFleetIdeDistribution631906CF",
        "DomainName"
       ]
      }
     ]
    ]
   }
  },
  "IdePassword": {
   "Value": {
    "Fn::GetAtt": [
     "IDEFleetIdePasswordExporterF99DE17D",
     "password"
    ]
   }
  }
 }
}



